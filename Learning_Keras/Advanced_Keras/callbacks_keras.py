# -*- coding: utf-8 -*-
"""Callbacks_Keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aZBZUEvVqaT808s_Bu09stFypRGA6LxU

# Keras Callbacks

## Callbacks

- A callback is an object (a class instance implementing specific methods) that is passed to the model in the call to fit and that is called by the model at various points during training. 
- It has access to all the available data about the state of the model and its performance, and it can take action: interrupt training, save a model, load a different weight set, or otherwise alter the state of the model.
- Here are some examples of ways you can use callbacks:
1. Model checkpointing — Saving the current weights of the model at different points during training.
2. Early stopping — Interrupting training when the validation loss is no longer
improving (and of course, saving the best model obtained during training).
3. Dynamically adjusting the value of certain parameters during training—Such as the learning rate of the optimizer.
4. Logging training and validation metrics during training, or visualizing the representations learned by the model as they’re updated

### ModelCheckpointing and EarlyStopping Callbacks

- You can use the EarlyStopping callback to interrupt training once a target metric being monitored has stopped improving for a fixed number of epochs. 
- For instance, this callback allows you to interrupt training as soon as you start overfitting, thus avoiding having to retrain your model for a smaller number of epochs. 
- This callback is typically used in combination with ModelCheckpoint, which lets you continually save the model during training (and, optionally, save only the current best model so far)
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
from keras.callbacks import ReduceLROnPlateau
from keras.callbacks import EarlyStopping
from keras.callbacks import ModelCheckpoint

callbacks_list = [EarlyStopping(monitor = 'val_acc', patience = 1, ), ModelCheckpoint('best_model.h5', monitor = 'val_acc', save_best_only=True)]  
# Interrupts training when accuracy has stopped improving for more than one epoch (that is, two epochs)

model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])
model.fit(x, y, epochs = 10, batch_size = 32, callbacks = callbacks_list, validation_data = (x_val, y_val))
# Note that because the callback will monitor validation loss and validation accuracy, you need to pass validation_data to the call to fit.

"""### ReduceLROnPlateau Callback

- Reducing the learning when the model is not able to push loss lower is a good idea.
- Divides the learning rate by 10 when triggered
The callback is triggered after the validation
loss has stopped improving for 10 epochs.
"""

callbacks_list = [(keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 10)]
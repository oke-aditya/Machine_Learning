{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BatchNormalization "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- Batch normalization is a type of layer (BatchNormalization in Keras) introduced\n",
    "in 2015 by Ioffe and Szegedy. \n",
    "- It can adaptively normalize data even as the mean and variance change over time during training. It works by internally maintaining an exponential moving average of the batch-wise mean and variance of the data seen during\n",
    "training. \n",
    "- The main effect of batch normalization is that it helps with gradient propagation—\n",
    "much like residual connections—and thus allows for deeper networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- The BatchNormalization layer is typically used after a convolutional or densely\n",
    "connected layer:\n",
    "\n",
    "- The BatchNormalization layer takes an axis argument, which specifies the feature\n",
    "axis that should be normalized. \n",
    "- This argument defaults to -1, the last axis in the input\n",
    "tensor. \n",
    "- This is the correct value when using Dense layers, Conv1D layers, RNN layers,\n",
    "and Conv2D layers with data_format set to \"channels_last\". \n",
    "- But in the niche use case of Conv2D layers with data_format set to \"channels_first\", the features axis is axis 1;\n",
    "the axis argument in BatchNormalization should accordingly be set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_model.add(layers.Conv2D(32, 3, activation = 'relu'))\n",
    "conv_model.add(BatchNormalization())\n",
    "\n",
    "dense_model.add(layers.Dense(32, activation = 'relu'))\n",
    "dense_model.add(BatchNormalization())"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depthwise Separable Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- The depthwise separable convolution layer does\n",
    "(SeparableConv2D). \n",
    "- This layer performs a spatial convolution on each channel of its\n",
    "input, independently, before mixing output channels via a pointwise convolution (a\n",
    "1 × 1 convolution),"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- This is equivalent to separating the learning of spatial features and the learning of channel-wise features, which makes a lot of\n",
    "sense if you assume that spatial locations in the input are highly correlated, but different\n",
    "channels are fairly independent. \n",
    "- It requires significantly fewer parameters and\n",
    "involves fewer computations, thus resulting in smaller, speedier models. And because\n",
    "it’s a more representationally efficient way to perform convolution, it tends to learn\n",
    "better representations using less data, resulting in better-performing models"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "- These advantages become especially important when you’re training small models\n",
    "from scratch on limited data.\n",
    "- When it comes to larger-scale models, depthwise separable convolutions are the basis\n",
    "of the Xception architecture, a high-performing convnet that comes packaged with\n",
    "Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 64\n",
    "width = 64\n",
    "channels = 3\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layers.SeparableConv2D(32, 3, activation = 'relu', input_shape = (height, width, channels)))\n",
    "model.add(layers.SeperableConv2D(64, 3, activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "model.add(layers.SeparableConv2D(64, 3, activation='relu'))\n",
    "model.add(layers.SeparableConv2D(128, 3, activation='relu'))\n",
    "model.add(layers.MaxPooling2D(2))\n",
    "\n",
    "model.add(layers.SeparableConv2D(64, 3, activation='relu'))\n",
    "model.add(layers.SeparableConv2D(128, 3, activation='relu'))\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nseparable_conv2d_1 (Separabl (None, 62, 62, 32)        155       \n_________________________________________________________________\nseparable_conv2d_2 (Separabl (None, 60, 60, 64)        2400      \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 30, 30, 64)        0         \n_________________________________________________________________\nseparable_conv2d_3 (Separabl (None, 28, 28, 64)        4736      \n_________________________________________________________________\nseparable_conv2d_4 (Separabl (None, 26, 26, 128)       8896      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 13, 13, 128)       0         \n_________________________________________________________________\nseparable_conv2d_5 (Separabl (None, 11, 11, 64)        9408      \n_________________________________________________________________\nseparable_conv2d_6 (Separabl (None, 9, 9, 128)         8896      \n_________________________________________________________________\nglobal_average_pooling2d_1 ( (None, 128)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 32)                4128      \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 38,949\nTrainable params: 38,949\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ]
}
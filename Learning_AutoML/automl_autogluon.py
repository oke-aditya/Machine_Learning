# -*- coding: utf-8 -*-
"""AutoML_AutoGluon

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19r3P9okXVlAGns9k7cf8spvPIZWy4-ah

# Using AutoGluon for AutoML

- This is a package developed by AWS.
- It is used for deep learning operations.
- Project details = [https://autogluon.mxnet.io/](https://autogluon.mxnet.io/)
- Uses YOLO v3 for object detection. Does a transfer learning.
- Used to automatically do image classification, object detection etc
- Work for text and tabular data as well.
- It uses the mxnet framework for deep learning.
- Can use even Neural Architecture Search.

# Install AutoGluon

- Please do not reverse the order of the cells.
- Execute in this way only
"""

! nvidia-smi

pip uninstall -y mkl

pip install --upgrade mxnet

pip install autogluon

pip install -U ipykernel

"""- Restart Colab Runtime, then execute remaining cells

# Time to use it
"""

import autogluon as ag

import pandas as pd
import numpy as np
import os, urllib

from autogluon import TabularPrediction as task

BASE_DIR = '/tmp'
OUTPUT_FILE = os.path.join(BASE_DIR, 'churn_data.csv')

churn_data = urllib.request.urlretrieve('https://raw.githubusercontent.com/srivatsan88/YouTubeLI/master/dataset/WA_Fn-UseC_-Telco-Customer-Churn.csv', OUTPUT_FILE)

churn_master_df = pd.read_csv(OUTPUT_FILE)

size = int(0.8 * len(churn_master_df))
train_df = churn_master_df[:size]
test_df = churn_master_df[size:]

test_df.shape

train_df.shape

train_df.head(3)

"""# Use the dataset object to train and predict

- Dataset can take pandas df, csv or dict.
- This will create an autogluon dataset
"""

train_data = task.Dataset(df=train_df)
test_data = task.Dataset(df=test_df)

train_data.head(3)

train_data.describe()

pred_column = 'Churn'
train_data[pred_column].describe()

predictor = task.fit(train_data=train_data, label=pred_column, eval_metric='accuracy')

y_test = test_data[pred_column]
test_data.drop(labels=[pred_column], axis=1, inplace=True)

test_data.head()

y_pred = predictor.predict(test_data)
performance = predictor.evaluate_predictions(y_test, y_pred, auxiliary_metrics=True)
print("Evaluation: ", performance)

print(predictor.problem_type)
print(predictor.feature_types)

#   It should have deleted customer ID. It does not take care of feature selection

predictor.predict_proba(test_data)

predictor.leaderboard()

predictor.fit_summary()

"""# Customizing hyper parameter tuning"""

hp_tune = True

rf_options = {
    'n_estimators' : 100,
}

gbm_options = {
    'num_boost_rounds' : 100,
    'num_leaves' : ag.space.Int(lower=6, upper=20, default=8)
}

hyperparameters = {'RF' : rf_options, 'GBM' : gbm_options}

time_limits = 2*60  # 2 mins
num_trials = 1  # Use higher if possible
search_strategy = "skopt" # Bayesian optimization

train_data = task.Dataset(df=train_df)
test_data = task.Dataset(df=test_df)

predictor = task.fit(train_data, tuning_data=test_data, label=pred_column, time_limits=time_limits, num_trials=num_trials,
                     hyperparameter_tune=hp_tune, hyperparameters=hyperparameters, search_strategy=search_strategy, nthreads_per_trial=1, 
                     ngpus_per_trial=1)

predictor.fit_summary()

predictor.leaderboard()
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to TorchScript.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mx4CF9VFRo59",
        "colab_type": "text"
      },
      "source": [
        "# Using TorchScript"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxiKYgKSRr1T",
        "colab_type": "text"
      },
      "source": [
        "This tutorial is an introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0aeF1j3O_2F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d253ea67-407c-4dc4-f5fc-06107ba7c0aa"
      },
      "source": [
        "import torch  # This is all you need to use both PyTorch and TorchScript!\n",
        "import torch.nn as nn\n",
        "print(torch.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.0+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmcpu8SgTIir",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "    Specific methods for converting PyTorch modules to TorchScript, our high-performance deployment runtime\n",
        "\n",
        "    Tracing an existing module\n",
        "    Using scripting to directly compile a module\n",
        "    How to compose both approaches\n",
        "    Saving and loading TorchScript modules\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8Jk2xkKQbGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCell(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    \n",
        "    def forward(self, x, h):\n",
        "        new_h = torch.tanh(x + h)\n",
        "        return new_h, new_h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiQzvqMGTc3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_cell = MyCell()\n",
        "x = torch.rand(3, 4)\n",
        "h = torch.rand(3, 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bavq2CvSTiB_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "c1b2bb96-aca0-4a12-bf97-4c9561e888e2"
      },
      "source": [
        "print(my_cell(x, h))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[0.5610, 0.0762, 0.8869, 0.6510],\n",
            "        [0.9313, 0.8051, 0.6170, 0.1723],\n",
            "        [0.7224, 0.7644, 0.7500, 0.6112]]), tensor([[0.5610, 0.0762, 0.8869, 0.6510],\n",
            "        [0.9313, 0.8051, 0.6170, 0.1723],\n",
            "        [0.7224, 0.7644, 0.7500, 0.6112]]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMhNQJMETpT7",
        "colab_type": "text"
      },
      "source": [
        "# Simple Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rqTx8QcTlXP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCell(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyCell, self).__init__()\n",
        "        self.linear = torch.nn.Linear(4, 4)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        new_h = torch.tanh(self.linear(x) + h)\n",
        "        return new_h, new_h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmoWMB5aTro4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_cell = MyCell()\n",
        "print(my_cell)\n",
        "print(my_cell(x, h))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WS1vaPuT29e",
        "colab_type": "text"
      },
      "source": [
        "# Basics of TorchScript\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haGYxsN7U3yn",
        "colab_type": "text"
      },
      "source": [
        "Torch Script provides us 2 ways of converting the Python code to low level code.\n",
        "\n",
        "1. Tracing\n",
        "2. Scripting\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSlYGXm_T4R-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Now let’s take our running example and see how we can apply TorchScript.\n",
        "\n",
        "In short, TorchScript provides tools to capture the definition of your model, even in light of the flexible and dynamic nature of PyTorch. Let’s begin by examining what we call tracing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzPNZmv2VA5v",
        "colab_type": "text"
      },
      "source": [
        "# Tracing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_iR81tLTsUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyCell(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyCell, self).__init__()\n",
        "        self.linear = torch.nn.Linear(4, 4)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        new_h = torch.tanh(self.linear(x) + h)\n",
        "        return new_h, new_h\n",
        "\n",
        "my_cell = MyCell()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyU1Jpg6T-ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, h = torch.rand(3, 4), torch.rand(3, 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb-5NvRVUCIH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "06f6dc91-8a71-4dfb-fad0-b698ab0d89ca"
      },
      "source": [
        "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
        "print(traced_cell)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyCell(\n",
            "  original_name=MyCell\n",
            "  (linear): Linear(original_name=Linear)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv3ww0dDUDpW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "6a0f1eec-c789-466f-8989-76351f2ed7f7"
      },
      "source": [
        "traced_cell(x, h)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.9313, -0.0346,  0.8589,  0.7222],\n",
              "         [ 0.4903, -0.0333,  0.8580,  0.7414],\n",
              "         [ 0.8313,  0.1273,  0.8266,  0.1831]], grad_fn=<TanhBackward>),\n",
              " tensor([[ 0.9313, -0.0346,  0.8589,  0.7222],\n",
              "         [ 0.4903, -0.0333,  0.8580,  0.7414],\n",
              "         [ 0.8313,  0.1273,  0.8266,  0.1831]], grad_fn=<TanhBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LePLDaUYUMNk",
        "colab_type": "text"
      },
      "source": [
        "We’ve rewinded a bit and taken the second version of our MyCell class. As before, we’ve instantiated it, but this time, we’ve called torch.jit.trace, passed in the Module, and passed in example inputs the network might see.\n",
        "\n",
        "What exactly has this done? It has invoked the Module, recorded the operations that occured when the Module was run, and created an instance of torch.jit.ScriptModule (of which TracedModule is an instance)\n",
        "\n",
        "TorchScript records its definitions in an Intermediate Representation (or IR), commonly referred to in Deep learning as a graph. We can examine the graph with the .graph property:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdd4dETBUEeg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "1fcc04a4-7808-46e5-af81-45cc421dab24"
      },
      "source": [
        "print(traced_cell.graph)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "graph(%self.1 : __torch__.MyCell,\n",
            "      %input : Float(3, 4),\n",
            "      %h : Float(3, 4)):\n",
            "  %19 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"linear\"](%self.1)\n",
            "  %21 : Tensor = prim::CallMethod[name=\"forward\"](%19, %input)\n",
            "  %12 : int = prim::Constant[value=1]() # <ipython-input-9-c84ad9de827c>:7:0\n",
            "  %13 : Float(3, 4) = aten::add(%21, %h, %12) # <ipython-input-9-c84ad9de827c>:7:0\n",
            "  %14 : Float(3, 4) = aten::tanh(%13) # <ipython-input-9-c84ad9de827c>:7:0\n",
            "  %15 : (Float(3, 4), Float(3, 4)) = prim::TupleConstruct(%14, %14)\n",
            "  return (%15)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2tAoIm4UVlM",
        "colab_type": "text"
      },
      "source": [
        "However, this is a very low-level representation and most of the information contained in the graph is not useful for end users. Instead, we can use the .code property to give a Python-syntax interpretation of the code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pD74E1tUPvW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "e13b07e1-4143-4b8f-c1b3-0328e3eaa89d"
      },
      "source": [
        "print(traced_cell.code)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "def forward(self,\n",
            "    input: Tensor,\n",
            "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
            "  _0 = torch.add((self.linear).forward(input, ), h, alpha=1)\n",
            "  _1 = torch.tanh(_0)\n",
            "  return (_1, _1)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCQxa3GWUadr",
        "colab_type": "text"
      },
      "source": [
        "# But Why TorchScript ??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGyEMsFXUc2x",
        "colab_type": "text"
      },
      "source": [
        "So why did we do all this? There are several reasons:\n",
        "\n",
        "TorchScript code can be invoked in its own interpreter, which is basically a restricted Python interpreter. This interpreter does not acquire the Global Interpreter Lock, and so many requests can be processed on the same instance simultaneously.\n",
        "\n",
        "This format allows us to save the whole model to disk and load it into another environment, such as in a server written in a language other than Python\n",
        "\n",
        "TorchScript gives us a representation in which we can do compiler optimizations on the code to provide more efficient execution\n",
        "\n",
        "TorchScript allows us to interface with many backend/device runtimes that require a broader view of the program than individual operators.\n",
        "\n",
        "We can see that invoking traced_cell produces the same results as the Python module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mht6BMpnUXfb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "70a3cad6-7658-46db-8035-1c45a309b576"
      },
      "source": [
        "print(my_cell(x, h))\n",
        "print(traced_cell(x, h))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([[ 0.9313, -0.0346,  0.8589,  0.7222],\n",
            "        [ 0.4903, -0.0333,  0.8580,  0.7414],\n",
            "        [ 0.8313,  0.1273,  0.8266,  0.1831]], grad_fn=<TanhBackward>), tensor([[ 0.9313, -0.0346,  0.8589,  0.7222],\n",
            "        [ 0.4903, -0.0333,  0.8580,  0.7414],\n",
            "        [ 0.8313,  0.1273,  0.8266,  0.1831]], grad_fn=<TanhBackward>))\n",
            "(tensor([[ 0.9313, -0.0346,  0.8589,  0.7222],\n",
            "        [ 0.4903, -0.0333,  0.8580,  0.7414],\n",
            "        [ 0.8313,  0.1273,  0.8266,  0.1831]], grad_fn=<TanhBackward>), tensor([[ 0.9313, -0.0346,  0.8589,  0.7222],\n",
            "        [ 0.4903, -0.0333,  0.8580,  0.7414],\n",
            "        [ 0.8313,  0.1273,  0.8266,  0.1831]], grad_fn=<TanhBackward>))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lB-73GheVDll",
        "colab_type": "text"
      },
      "source": [
        "# Scripting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uRXshtFVO2q",
        "colab_type": "text"
      },
      "source": [
        "Using Scripting to Convert Modules\n",
        "\n",
        "There’s a reason we used version two of our module, and not the one with the control-flow-laden submodule. Let’s examine that now"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI_kJRKDUp9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDecisionGate(torch.nn.Module):\n",
        "    def forward(self, x):\n",
        "        if x.sum() > 0:\n",
        "            return x\n",
        "        else:\n",
        "            return -x\n",
        "\n",
        "class MyCell(torch.nn.Module):\n",
        "    def __init__(self, dg):\n",
        "        super(MyCell, self).__init__()\n",
        "        self.dg = dg\n",
        "        self.linear = torch.nn.Linear(4, 4)\n",
        "\n",
        "    def forward(self, x, h):\n",
        "        new_h = torch.tanh(self.dg(self.linear(x)) + h)\n",
        "        return new_h, new_h\n",
        "\n",
        "my_cell = MyCell(MyDecisionGate())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsa_b_9rVXsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "ae6d262b-7465-4e38-fff6-23ba4146ab58"
      },
      "source": [
        "traced_cell = torch.jit.trace(my_cell, (x, h))\n",
        "# print(traced_cell.code)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FulYfWnVZdO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "c84facc7-1af2-4378-d362-968027ddf982"
      },
      "source": [
        "print(traced_cell.code)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "def forward(self,\n",
            "    input: Tensor,\n",
            "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
            "  _0 = self.dg\n",
            "  _1 = (self.linear).forward(input, )\n",
            "  _2 = (_0).forward(_1, )\n",
            "  _3 = torch.tanh(torch.add(_1, h, alpha=1))\n",
            "  return (_3, _3)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GiK-k5yVgMT",
        "colab_type": "text"
      },
      "source": [
        "Looking at the .code output, we can see that the if-else branch is nowhere to be found! \n",
        "\n",
        "Why? Tracing does exactly what we said it would: run the code, record the operations that happen and construct a ScriptModule that does exactly that. Unfortunately, things like control flow are erased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amnm4HfRVlUO",
        "colab_type": "text"
      },
      "source": [
        "How can we faithfully represent this module in TorchScript? We provide a script compiler, which does direct analysis of your Python source code to transform it into TorchScript. Let’s convert MyDecisionGate using the script compiler:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93LGHB9zVd5e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scripted_gate = torch.jit.script(MyDecisionGate())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6LoNFRZVod1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_cell = MyCell(scripted_gate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4W6rNCiVplY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traced_cell = torch.jit.script(my_cell)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZKXCzi-Vqnr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "7fddb65e-86db-48ce-bb12-dd52da6782b7"
      },
      "source": [
        "print(traced_cell.code)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "def forward(self,\n",
            "    x: Tensor,\n",
            "    h: Tensor) -> Tuple[Tensor, Tensor]:\n",
            "  _0 = (self.dg).forward((self.linear).forward(x, ), )\n",
            "  new_h = torch.tanh(torch.add(_0, h, alpha=1))\n",
            "  return (new_h, new_h)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_TG0qgcVq0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "0d38ec20-6d73-4327-aa76-2d8e72c46e0b"
      },
      "source": [
        "# New inputs\n",
        "x, h = torch.rand(3, 4), torch.rand(3, 4)\n",
        "traced_cell(x, h)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.1735, 0.3896, 0.1230, 0.9110],\n",
              "         [0.4432, 0.7596, 0.7057, 0.8592],\n",
              "         [0.1503, 0.7608, 0.0539, 0.8040]], grad_fn=<TanhBackward>),\n",
              " tensor([[0.1735, 0.3896, 0.1230, 0.9110],\n",
              "         [0.4432, 0.7596, 0.7057, 0.8592],\n",
              "         [0.1503, 0.7608, 0.0539, 0.8040]], grad_fn=<TanhBackward>))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSW8nKeXVw3T",
        "colab_type": "text"
      },
      "source": [
        "# Mixing Scripting and Tracing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p38EEhtvWLXo",
        "colab_type": "text"
      },
      "source": [
        "Some situations call for using tracing rather than scripting (e.g. a module has many architectural decisions that are made based on constant Python values that we would like to not appear in TorchScript). \n",
        "\n",
        "In this case, scripting can be composed with tracing: torch.jit.script will inline the code for a traced module, and tracing will inline the code for a scripted module."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1Ic3vK4VumG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyRNNLoop(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyRNNLoop, self).__init__()\n",
        "        self.cell = torch.jit.trace(MyCell(scripted_gate), (x, h))\n",
        "\n",
        "    def forward(self, xs):\n",
        "        h, y = torch.zeros(3, 4), torch.zeros(3, 4)\n",
        "        for i in range(xs.size(0)):\n",
        "            y, h = self.cell(xs[i], h)\n",
        "        return y, h\n",
        "\n",
        "rnn_loop = torch.jit.script(MyRNNLoop())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HS3fwQdmWnRn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "8f0f743c-8bb3-4892-d8ff-10b3d25feafa"
      },
      "source": [
        "print(rnn_loop.code)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "def forward(self,\n",
            "    xs: Tensor) -> Tuple[Tensor, Tensor]:\n",
            "  h = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None)\n",
            "  y = torch.zeros([3, 4], dtype=None, layout=None, device=None, pin_memory=None)\n",
            "  y0 = y\n",
            "  h0 = h\n",
            "  for i in range(torch.size(xs, 0)):\n",
            "    _0 = (self.cell).forward(torch.select(xs, 0, i), h0, )\n",
            "    y1, h1, = _0\n",
            "    y0, h0 = y1, h1\n",
            "  return (y0, h0)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeT0bcVVWpT1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "b444fdb1-fd3a-4514-c6b6-c4e51c041df4"
      },
      "source": [
        "class WrapRNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WrapRNN, self).__init__()\n",
        "        self.loop = torch.jit.script(MyRNNLoop())\n",
        "\n",
        "    def forward(self, xs):\n",
        "        y, h = self.loop(xs)\n",
        "        return torch.relu(y)\n",
        "\n",
        "traced = torch.jit.trace(WrapRNN(), (torch.rand(10, 3, 4)))\n",
        "print(traced.code)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "def forward(self,\n",
            "    argument_1: Tensor) -> Tensor:\n",
            "  _0, h, = (self.loop).forward(argument_1, )\n",
            "  return torch.relu(h)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSKtr9KcWu4a",
        "colab_type": "text"
      },
      "source": [
        "This way, scripting and tracing can be used when the situation calls for each of them and used together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5mCN5KhWvck",
        "colab_type": "text"
      },
      "source": [
        "# Saving and Loading Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZKFx-RFWyMf",
        "colab_type": "text"
      },
      "source": [
        "We provide APIs to save and load TorchScript modules to/from disk in an archive format. \n",
        "\n",
        "This format includes code, parameters, attributes, and debug information, meaning that the archive is a freestanding representation of the model that can be loaded in an entirely separate process. Let’s save and load our wrapped RNN module:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcmJgiyaWr4A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traced.save('wrapped_rnn.zip')\n",
        "\n",
        "loaded = torch.jit.load('wrapped_rnn.zip')\n",
        "\n",
        "print(loaded)\n",
        "print(loaded.code)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
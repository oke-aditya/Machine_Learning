{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parallel_GPU_PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05GcCZrxgJ8B",
        "colab_type": "text"
      },
      "source": [
        "# Parallel GPUs with nn.DataParallel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-43PTKaugchw",
        "colab_type": "text"
      },
      "source": [
        "In this tutorial, we will learn how to use multiple GPUs using DataParallel.\n",
        "\n",
        "It’s very easy to use GPUs with PyTorch. You can put the model on a GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pegS8-ltfh1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4dAryHGgjXK",
        "colab_type": "text"
      },
      "source": [
        "Please note that just calling my_tensor.to(device) returns a new copy of my_tensor on GPU instead of rewriting my_tensor. You need to assign it to a new tensor and use that tensor on the GPU.\n",
        "\n",
        "It’s natural to execute your forward, backward propagations on multiple GPUs. However, Pytorch will only use one GPU by default. You can easily run your operations on multiple GPUs by making your model run parallelly using DataParallel:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frYFeVWVgfaa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Parameters and DataLoaders\n",
        "input_size = 5\n",
        "output_size = 2\n",
        "\n",
        "batch_size = 30\n",
        "data_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82QfG5xpgmxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsoG_OaNgq4Z",
        "colab_type": "text"
      },
      "source": [
        "# Dummy DataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "719gcOwAgpLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RandomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, size, length):\n",
        "        self.len = length\n",
        "        self.data = torch.randn(length, size)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9tgVyW_g2SA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "randn_loader = DataLoader(dataset=RandomDataset(input_size, data_size), shuffle=True, batch_size=batch_size, )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw5y3finhGsu",
        "colab_type": "text"
      },
      "source": [
        "# Simple Model\n",
        "\n",
        "For the demo, our model just gets an input, performs a linear operation, and gives an output. However, you can use DataParallel on any model (CNN, RNN, Capsule Net etc.)\n",
        "\n",
        "We’ve placed a print statement inside the model to monitor the size of input and output tensors. Please pay attention to what is printed at batch rank 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L0TbiD-hEEB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "    # Our model\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(Model, self).__init__()\n",
        "        self.fc = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.fc(input)\n",
        "        print(\"\\tIn Model: input size\", input.size(),\n",
        "              \"output size\", output.size())\n",
        "\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5rf0fU0hLZ_",
        "colab_type": "text"
      },
      "source": [
        "# Create Model and DataParallel\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqB5Uo44hMmp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "This is the core part of the tutorial. \n",
        "\n",
        "First, we need to make a model instance and check if we have multiple GPUs. If we have multiple GPUs, we can wrap our model using nn.DataParallel. Then we can put our model on GPUs by model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSSw2-1VhTf-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "13acc6c7-e751-4dfc-bac0-ddbf12ceac47"
      },
      "source": [
        "print(torch.cuda.device_count())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQew_07JhJDC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "cd5a13c9-e6ff-4f0d-ab8d-56be0793f8ca"
      },
      "source": [
        "model = Model(input_size, output_size)\n",
        "if torch.cuda.device_count() > 1:\n",
        "  print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
        "  # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n",
        "  model = nn.DataParallel(model)\n",
        "\n",
        "model.to(device)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Model(\n",
              "  (fc): Linear(in_features=5, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe4CsaSzhXhP",
        "colab_type": "text"
      },
      "source": [
        "# Run the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhWKP84IhQLW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "a3c223eb-b10a-410f-e866-954eac67e27c"
      },
      "source": [
        "for data in randn_loader:\n",
        "    input = data.to(device)\n",
        "    output = model(input)\n",
        "    print(\"Outside: input size\", input.size(),\n",
        "          \"output_size\", output.size())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tIn Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\n",
            "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
            "\tIn Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\n",
            "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
            "\tIn Model: input size torch.Size([30, 5]) output size torch.Size([30, 2])\n",
            "Outside: input size torch.Size([30, 5]) output_size torch.Size([30, 2])\n",
            "\tIn Model: input size torch.Size([10, 5]) output size torch.Size([10, 2])\n",
            "Outside: input size torch.Size([10, 5]) output_size torch.Size([10, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oegs3kbYhgmk",
        "colab_type": "text"
      },
      "source": [
        "# Results\n",
        "\n",
        "If you have no GPU or one GPU, when we batch 30 inputs and 30 outputs, the model gets 30 and outputs 30 as expected. But if you have multiple GPUs, then you can get results different."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO599-8ehmUU",
        "colab_type": "text"
      },
      "source": [
        "# Summary\n",
        "\n",
        "DataParallel splits your data automatically and sends job orders to multiple models on several GPUs. After each model finishes their job, DataParallel collects and merges the results before returning it to you."
      ]
    }
  ]
}
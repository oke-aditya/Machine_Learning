{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Use Pytorch Lightning with W&B.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd5XiXj1EOs1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install pytorch_lightning\n",
        "!pip install --upgrade wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c7jIxMTEYoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pytorch_lightning as pl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cabOCPa4qL4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Configure the sweep â€“ specify the parameters to search through, the search strategy, the optimization metric et all.\n",
        "sweep_config = {\n",
        "    'method': 'random', #grid, random\n",
        "    'metric': {\n",
        "      'name': 'accuracy',\n",
        "      'goal': 'maximize'   \n",
        "    },\n",
        "    'parameters': {\n",
        "\n",
        "        'learning_rate': {\n",
        "            'values': [0.1, 0.01, 0.001]\n",
        "        },\n",
        "        'optimizer': {\n",
        "            'values': ['adam', 'sgd']\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkLv0LDq3GPz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.nn import functional as F\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision import datasets, transforms\n",
        "import os\n",
        "import wandb\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.logging.wandb import WandbLogger\n",
        "config_defaults = {\n",
        "        'learning_rate': 0.1,\n",
        "        'optimizer': 'adam',\n",
        "    }\n",
        "\n",
        "class LightningMNISTClassifier(pl.LightningModule):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(LightningMNISTClassifier, self).__init__()\n",
        "\n",
        "    # mnist images are (1, 28, 28) (channels, width, height) \n",
        "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
        "    self.layer_2 = torch.nn.Linear(128, 256)\n",
        "    self.layer_3 = torch.nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "      batch_size, channels, width, height = x.size()\n",
        "\n",
        "      # (b, 1, 28, 28) -> (b, 1*28*28)\n",
        "      x = x.view(batch_size, -1)\n",
        "\n",
        "      # layer 1\n",
        "      x = self.layer_1(x)\n",
        "      x = torch.relu(x)\n",
        "\n",
        "      # layer 2\n",
        "      x = self.layer_2(x)\n",
        "      x = torch.relu(x)\n",
        "\n",
        "      # layer 3\n",
        "      x = self.layer_3(x)\n",
        "\n",
        "      # probability distribution over labels\n",
        "      x = torch.log_softmax(x, dim=1)\n",
        "\n",
        "      return x\n",
        "\n",
        "  def cross_entropy_loss(self, logits, labels):\n",
        "    return F.nll_loss(logits, labels)\n",
        "    \n",
        "  def training_step(self, train_batch, batch_idx):\n",
        "      x, y = train_batch\n",
        "      logits = self.forward(x)   # we already defined forward and loss in the lightning module. We'll show the full code next\n",
        "      loss = self.cross_entropy_loss(logits, y)\n",
        "\n",
        "      logs = {'train_loss': loss}\n",
        "      return {'loss': loss, 'log': logs}\n",
        "\n",
        "  def validation_step(self, val_batch, batch_idx):\n",
        "      x, y = val_batch\n",
        "      logits = self.forward(x)\n",
        "      loss = self.cross_entropy_loss(logits, y)\n",
        "      return {'val_loss': loss}\n",
        "\n",
        "  def validation_end(self, outputs):\n",
        "      # outputs is an array with what you returned in validation_step for each batch\n",
        "      # outputs = [{'loss': batch_0_loss}, {'loss': batch_1_loss}, ..., {'loss': batch_n_loss}]\n",
        "      \n",
        "      avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "      tensorboard_logs = {'val_loss': avg_loss}\n",
        "      return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n",
        "\n",
        "  def prepare_data(self):\n",
        "    # prepare transforms standard to MNIST\n",
        "    MNIST(os.getcwd(), train=True, download=True)\n",
        "    MNIST(os.getcwd(), train=False, download=True)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "    mnist_train = MNIST(os.getcwd(), train=True, download=False, \n",
        "                        transform=transform)\n",
        "    self.mnist_train, self.mnist_val = random_split(mnist_train, [55000, 5000])\n",
        "\n",
        "    mnist_train = DataLoader(self.mnist_train, batch_size=32)\n",
        "    return mnist_train\n",
        "  def val_dataloader(self):\n",
        "    mnist_val = DataLoader(self.mnist_val, batch_size=32)\n",
        "    return mnist_val\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    transform=transforms.Compose([transforms.ToTensor(), \n",
        "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
        "    mnist_test = MNIST(os.getcwd(), train=False, download=False, \n",
        "                       transform=transform)\n",
        "    mnist_test = DataLoader(mnist_test, batch_size=32)\n",
        "    return mnist_test\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    # the lightningModule HAS the parameters (remember that we had the __init__ and forward method but we're just not showing it here)\n",
        "\n",
        "    optimizer = torch.optim.Adam(self.parameters(),0.001)\n",
        "    #optimizer =  torch.optim.SGD(self.parameters(),lr=0.01)\n",
        "    return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V_rs9wO5tm4",
        "colab_type": "text"
      },
      "source": [
        "# Install NVIDIA apex for 16-bit precision"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwdzD-8-51Zo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%writefile setup.sh\n",
        "\n",
        "export CUDA_HOME=/usr/local/cuda-10.1\n",
        "git clone https://github.com/NVIDIA/apex\n",
        "pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ./apex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyRj_HE75_Ol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!sh setup.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NK_8U1T4FG0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "wandb_logger = WandbLogger(name='TestRun-16-bit-adam-0.001',project='pytorchlightning')\n",
        "#wandb_logger = WandbLogger(name='Adam-32-0.01',project='pytorchlightning')\n",
        "#wandb_logger = WandbLogger(name='Adam-64-0.001',project='pytorchlightning')\n",
        "#wandb_logger = WandbLogger(name='Adam-64-0.01',project='pytorchlightning')\n",
        "#wandb_logger = WandbLogger(name='sgd-32-0.001',project='pytorchlightning')\n",
        "#wandb_logger = WandbLogger(name='sgd-64-0.001',project='pytorchlightning')\n",
        "#wandb_logger = WandbLogger(name='sgd-32-0.01',project='pytorchlightning')\n",
        "#wandb_logger = WandbLogger(name='sgd-64-0.01',project='pytorchlightning')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yETJckTi91P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LightningMNISTClassifier()\n",
        "model.prepare_data()\n",
        "model.train_dataloader()\n",
        "#Change the GPU number to the number of gpus you wish to use\n",
        "trainer = pl.Trainer(max_epochs = 100,logger= wandb_logger, gpus=1, distributed_backend='dp',early_stop_callback=True, amp_level='O1',precision=16)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caFIN3KgjWb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "  trainer.fit(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHBhQxuSyj2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZE3Jglxw_SQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer.save_checkpoint('EarlyStoppingADam-32-0.001.pth')\n",
        "wandb.save('EarlyStoppingADam-32-0.001.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZCBLfELDmRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wandb.restore('EarlyStoppingADam-32-0.001.pth')\n",
        "model.load_from_checkpoint('EarlyStoppingADam-32-0.001.pth')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1igQG6nghz6Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sweep_id = wandb.sweep(sweep_config,project='pytorchlightning')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehua3RBSh7jr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wandb.agent(sweep_id, train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FttRdl2vGQVY",
        "colab_type": "text"
      },
      "source": [
        "# Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh1OTIctH7cM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "pytorch_model = MNISTClassifier()\n",
        "optimizer = torch.optim.Adam(pytorch_model.parameters(), lr=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F-s9oqMHWcX",
        "colab_type": "text"
      },
      "source": [
        "In PyTorch, this dataloading can be done anywhere in your main training file... In PyTorch Lightning it is done in the three specific methods of the LightningModule.\n",
        "\n",
        "train_dataloader()\n",
        "val_dataloader()\n",
        "test_dataloader()\n",
        "And a fourth method meant for data preparation/downloading.\n",
        "\n",
        "prepare_data()\n",
        "Lightning takes this approach so that every model implemented with Lightning follows the SAME structure. This makes code extremely readable and organized.\n",
        "\n",
        "This means that when you run into a Github project that uses Lightning you'll be able to know exactly where the data processing/loading happened."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh7Zhs7aHZ2f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MNISTClassifierPL(pl.LightningModule):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(MNISTClassifierPL, self).__init__()\n",
        "\n",
        "    # mnist images (1, 28, 28) => (channels, width, height) \n",
        "    self.layer_1 = torch.nn.Linear(28 * 28, 128)\n",
        "    self.layer_2 = torch.nn.Linear(128, 256)\n",
        "    self.layer_3 = torch.nn.Linear(256, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, channels, width, height = x.size()\n",
        "    x = x.view(batch_size, -1)\n",
        "\n",
        "    x = self.layer_1(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    x = self.layer_2(x)\n",
        "    x = torch.relu(x)\n",
        "\n",
        "    x = self.layer_3(x)\n",
        "\n",
        "    out = torch.log_softmax(x, dim=1)\n",
        "\n",
        "    return out\n",
        "    \n",
        "  def prepare_data(self):\n",
        "    # prepare transforms standard to MNIST\n",
        "    MNIST(os.getcwd(), train=True, download=True)\n",
        "    MNIST(os.getcwd(), train=False, download=True)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    transform=transforms.Compose([transforms.ToTensor(), \n",
        "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
        "    mnist_train = MNIST(os.getcwd(), train=True, download=False, \n",
        "                        transform=transform)\n",
        "    self.mnist_train, self.mnist_val = random_split(self.mnist_train, [55000, 5000])\n",
        "\n",
        "    mnist_train = DataLoader(mnist_train, batch_size=64)\n",
        "    return mnist_train\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    mnist_val = DataLoader(self.mnist_val, batch_size=64)\n",
        "    return mnist_val\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    transform=transforms.Compose([transforms.ToTensor(), \n",
        "                                  transforms.Normalize((0.1307,), (0.3081,))])\n",
        "    mnist_test = MNIST(os.getcwd(), train=False, download=False, \n",
        "                       transform=transform)\n",
        "    mnist_test = DataLoader(mnist_test, batch_size=64)\n",
        "    return mnist_test\n",
        "\n",
        "  #The optimizer code is the same for Lightning, except that it is added to the function configure_optimizers() in the LightningModule.\n",
        "\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "      optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
        "      return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
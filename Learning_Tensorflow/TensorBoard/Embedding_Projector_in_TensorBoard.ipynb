{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Embedding Projector in TensorBoard.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVcQc4hwPTzh",
        "colab_type": "text"
      },
      "source": [
        "# Using Embedding Projector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGFR_sR1PWmA",
        "colab_type": "text"
      },
      "source": [
        "Using the TensorBoard Embedding Projector, you can graphically represent high dimensional embeddings. This can be helpful in visualizing, examining, and understanding your embedding layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex_IB99QOkIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorboard.plugins import projector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfHhIY74PYwZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "fe0290af-aeb9-4b3f-b662-96f512462333"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYeyo45mPeT3",
        "colab_type": "text"
      },
      "source": [
        "# IMDB Data\n",
        "\n",
        "We will be using a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
        "\n",
        "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word. Later in the tutorial, we will be removing this row from the visualization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvj9ovFsPcS_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_data, test_data), info = tfds.load(\n",
        "    \"imdb_reviews/subwords8k\",\n",
        "    split=(tfds.Split.TRAIN, tfds.Split.TEST),\n",
        "    with_info=True,\n",
        "    as_supervised=True,\n",
        ")\n",
        "encoder = info.features[\"text\"].encoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2zvywizPjRk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batches = train_data.shuffle(1000).padded_batch(10)\n",
        "test_batches = test_data.shuffle(1000).padded_batch(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUOC5KGKPuzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batch, train_labels = next(iter(train_batches))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eglMaxv4Q9jx",
        "colab_type": "text"
      },
      "source": [
        "# Create Embeddings\n",
        "\n",
        "A Keras Embedding Layer can be used to train an embedding for each word in your volcabulary. Each word (or sub-word in this case) will be associated with a 16-dimensional vector (or embedding) that will be trained by the model.\n",
        "\n",
        "See this tutorial to learn more about word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KffHIZAPQ7Pp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create an embedding layer\n",
        "embedding_dim = 16\n",
        "embedding = tf.keras.layers.Embedding(encoder.vocab_size, embedding_dim)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             embedding,\n",
        "                             tf.keras.layers.GlobalAveragePooling1D(),\n",
        "                             tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "                             tf.keras.layers.Dense(1),\n",
        "\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2ekBlbRRMRB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "82603850-ae93-46c2-f16d-74de28db2032"
      },
      "source": [
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "\n",
        "# Train model\n",
        "history = model.fit(\n",
        "    train_batches, epochs=10, validation_data=test_batches, validation_steps=20\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2500/2500 [==============================] - 14s 5ms/step - loss: 0.5206 - accuracy: 0.6832 - val_loss: 0.3595 - val_accuracy: 0.8400\n",
            "Epoch 2/10\n",
            "2500/2500 [==============================] - 14s 5ms/step - loss: 0.2895 - accuracy: 0.8821 - val_loss: 0.2673 - val_accuracy: 0.8950\n",
            "Epoch 3/10\n",
            "2500/2500 [==============================] - 14s 5ms/step - loss: 0.2325 - accuracy: 0.9082 - val_loss: 0.3174 - val_accuracy: 0.8500\n",
            "Epoch 4/10\n",
            "2500/2500 [==============================] - 14s 6ms/step - loss: 0.2002 - accuracy: 0.9226 - val_loss: 0.2599 - val_accuracy: 0.9050\n",
            "Epoch 5/10\n",
            "2500/2500 [==============================] - 13s 5ms/step - loss: 0.1780 - accuracy: 0.9342 - val_loss: 0.3430 - val_accuracy: 0.8600\n",
            "Epoch 6/10\n",
            "2500/2500 [==============================] - 14s 5ms/step - loss: 0.1592 - accuracy: 0.9409 - val_loss: 0.4788 - val_accuracy: 0.8150\n",
            "Epoch 7/10\n",
            "2500/2500 [==============================] - 13s 5ms/step - loss: 0.1435 - accuracy: 0.9483 - val_loss: 0.6057 - val_accuracy: 0.7900\n",
            "Epoch 8/10\n",
            "2500/2500 [==============================] - 14s 5ms/step - loss: 0.1327 - accuracy: 0.9524 - val_loss: 0.4027 - val_accuracy: 0.8500\n",
            "Epoch 9/10\n",
            "2500/2500 [==============================] - 13s 5ms/step - loss: 0.1199 - accuracy: 0.9581 - val_loss: 0.4705 - val_accuracy: 0.8500\n",
            "Epoch 10/10\n",
            "2500/2500 [==============================] - 13s 5ms/step - loss: 0.1098 - accuracy: 0.9628 - val_loss: 0.5468 - val_accuracy: 0.8400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI_dcL51RZak",
        "colab_type": "text"
      },
      "source": [
        "# Saving data for TensorBoard\n",
        "\n",
        "TensorBoard reads tensors and metadata from your tensorflow projects from the logs in the specified log_dir directory. For this tutorial, we will be using /logs/imdb-example/.\n",
        "\n",
        "In order to visualize this data, we will be saving a checkpoint to that directory, along with metadata to understand which layer to visualize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWMMfCmhROuc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up a logs directory, so Tensorboard knows where to look for files\n",
        "log_dir='/logs/imdb-example/'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# Save Labels separately on a line-by-line manner.\n",
        "with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
        "  for subwords in encoder.subwords:\n",
        "    f.write(\"{}\\n\".format(subwords))\n",
        "  # Fill in the rest of the labels with \"unknown\"\n",
        "  for unknown in range(1, encoder.vocab_size - len(encoder.subwords)):\n",
        "    f.write(\"unknown #{}\\n\".format(unknown))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FgfV0lKRi2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the weights we want to analyse as a variable. Note that the first\n",
        "# value represents any unknown word, which is not in the metadata, so\n",
        "# we will remove that value.\n",
        "weights = tf.Variable(model.layers[0].get_weights()[0][1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbZWUHXuThK1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8e1b06db-2932-4017-a453-52a1ee55ac6b"
      },
      "source": [
        "# Create a checkpoint from embedding, the filename and key are\n",
        "# name of the tensor.\n",
        "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
        "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/logs/imdb-example/embedding.ckpt-1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmimE6TeTwcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up config\n",
        "config = projector.ProjectorConfig()\n",
        "embedding = config.embeddings.add()\n",
        "# The name of the tensor will be suffixed by `/.ATTRIBUTES/VARIABLE_VALUE`\n",
        "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
        "embedding.metadata_path = 'metadata.tsv'\n",
        "projector.visualize_embeddings(log_dir, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sL92oDhHT2GZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorboard --logdir /logs/imdb-example/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbM8vSt8T9QJ",
        "colab_type": "text"
      },
      "source": [
        "# Analysis\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46MaEKV2T-4c",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "The TensorBoard Projector is a great tool for analyzing your data and seeing embedding values relative to each other. \n",
        "\n",
        "The dashboard allows searching for specific terms, and highlights words that are nearby in the embedding space. \n",
        "\n",
        "From this example we can see that Wes Anderson and Alfred Hitchcock are both rather neutral terms, but that they are referenced in different contexts."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpN4X6keUEij",
        "colab_type": "text"
      },
      "source": [
        "Hitchcock is closer associated to words like nightmare, which likely relates to his work in horror movies. While Anderson is closer to the word heart, reflecting his heartwarming style."
      ]
    }
  ]
}
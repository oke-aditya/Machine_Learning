{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "distributed_training",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BE5mgPkuaHZn",
        "colab_type": "text"
      },
      "source": [
        "# Distributed training with TensorFlow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDnIgHSRae80",
        "colab_type": "text"
      },
      "source": [
        "tf.distribute.Strategy is a TensorFlow API to distribute training across multiple GPUs, multiple machines or TPUs. Using this API, you can distribute your existing models and training code with minimal code changes.\n",
        "\n",
        "tf.distribute.Strategy has been designed with these key goals in mind:\n",
        "\n",
        "    Easy to use and support multiple user segments, including researchers, ML engineers, etc.\n",
        "    Provide good performance out of the box.\n",
        "    Easy switching between strategies.\n",
        "\n",
        "tf.distribute.Strategy can be used with a high-level API like Keras, and can also be used to distribute custom training loops (and, in general, any computation using TensorFlow).\n",
        "\n",
        "In TensorFlow 2.0, you can execute your programs eagerly, or in a graph using tf.function. tf.distribute.Strategy intends to support both these modes of execution. Although we discuss training most of the time in this guide, this API can also be used for distributing evaluation and prediction on different platforms.\n",
        "\n",
        "You can use tf.distribute.Strategy with very few changes to your code, because we have changed the underlying components of TensorFlow to become strategy-aware. This includes variables, layers, models, optimizers, metrics, summaries, and checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_CLx3vCaEMx",
        "colab_type": "code",
        "outputId": "64a3702e-45a6-48aa-f6f6-6d8fcd4f61c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a3g6QhdalCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11UiVvoZapP6",
        "colab_type": "text"
      },
      "source": [
        "# GPU Strategies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxOMnXMJapMh",
        "colab_type": "text"
      },
      "source": [
        "tf.distribute.Strategy intends to cover a number of use cases along different axes. Some of these combinations are currently supported and others will be added in the future. Some of these axes are:\n",
        "\n",
        "- Synchronous vs asynchronous training: These are two common ways of distributing training with data parallelism. In sync training, all workers train over different slices of input data in sync, and aggregating gradients at each step. In async training, all workers are independently training over the input data and updating variables asynchronously. Typically sync training is supported via all-reduce and async through parameter server architecture.\n",
        "- Hardware platform: You may want to scale your training onto multiple GPUs on one machine, or multiple machines in a network (with 0 or more GPUs each), or on Cloud TPUs.\n",
        "\n",
        "In order to support these use cases, there are six strategies available. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28QfM-sFa31Y",
        "colab_type": "text"
      },
      "source": [
        "## Mirrored Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S99F_FdJa7dD",
        "colab_type": "text"
      },
      "source": [
        "tf.distribute.MirroredStrategy supports synchronous distributed training on multiple GPUs on one machine. \n",
        "\n",
        "It creates one replica per GPU device. Each variable in the model is mirrored across all the replicas. \n",
        "\n",
        "Together, these variables form a single conceptual variable called MirroredVariable. These variables are kept in sync with each other by applying identical updates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZC7qSHkbF8a",
        "colab_type": "text"
      },
      "source": [
        "Efficient all-reduce algorithms are used to communicate the variable updates across the devices. All-reduce aggregates tensors across all the devices by adding them up, and makes them available on each device. \n",
        "\n",
        "Itâ€™s a fused algorithm that is very efficient and can reduce the overhead of synchronization significantly. \n",
        "\n",
        "There are many all-reduce algorithms and implementations available, depending on the type of communication available between devices. By default, it uses NVIDIA NCCL as the all-reduce implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wE6zXrramWj",
        "colab_type": "code",
        "outputId": "3c2270d7-752f-43b7-ce0c-191a63fabb09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "mirrored_strategy = tf.distribute.MirroredStrategy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzkq6QqmbWlh",
        "colab_type": "text"
      },
      "source": [
        "This will create a MirroredStrategy instance which will use all the GPUs that are visible to TensorFlow, and use NCCL as the cross device communication.\n",
        "\n",
        "If you wish to use only some of the GPUs on your machine, you can do so like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZDnCJ3ZbSJW",
        "colab_type": "code",
        "outputId": "11c0f9f3-a8ee-49ed-d82e-5625a253edd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "mirrored_strategy = tf.distribute.MirroredStrategy(devices=[\"/gpu:0\"])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi0BZCELbj-Q",
        "colab_type": "text"
      },
      "source": [
        "If you wish to override the cross device communication, you can do so using the cross_device_ops argument by supplying an instance of tf.distribute.CrossDeviceOps. \n",
        "\n",
        "Currently, tf.distribute.HierarchicalCopyAllReduce and tf.distribute.ReductionToOneDevice are two options other than tf.distribute.NcclAllReduce which is the default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-ZCO3y-beKv",
        "colab_type": "code",
        "outputId": "ac46c072-d397-49e8-9466-a750d678ea52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "mirrored_strategy = tf.distribute.MirroredStrategy(cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK0GCxeYbzZO",
        "colab_type": "text"
      },
      "source": [
        "## Central Storage Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KbmAHb5b6Gc",
        "colab_type": "text"
      },
      "source": [
        "tf.distribute.experimental.CentralStorageStrategy does synchronous training as well. \n",
        "\n",
        "Variables are not mirrored, instead they are placed on the CPU and operations are replicated across all local GPUs. If there is only one GPU, all variables and operations will be placed on that GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRx_FPuUbwDU",
        "colab_type": "code",
        "outputId": "ca9126df-3c2f-477b-8dab-9ed33ab1eca2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 56
        }
      },
      "source": [
        "central_storage_strategy = tf.distribute.experimental.CentralStorageStrategy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:ParameterServerStrategy (CentralStorageStrategy if you are using a single machine) with compute_devices = ('/device:GPU:0',), variable_device = '/device:GPU:0'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKRdOFD3cH5L",
        "colab_type": "text"
      },
      "source": [
        "This will create a CentralStorageStrategy instance which will use all visible GPUs and CPU. Update to variables on replicas will be aggregated before being applied to variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1VM7xBAcJF1",
        "colab_type": "text"
      },
      "source": [
        "## MultiWorkerMirroredStrategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWlBceGDcWD3",
        "colab_type": "text"
      },
      "source": [
        "tf.distribute.experimental.MultiWorkerMirroredStrategy is very similar to MirroredStrategy. It implements synchronous distributed training across multiple workers, each with potentially multiple GPUs. Similar to MirroredStrategy, it creates copies of all variables in the model on each device across all workers.\n",
        "\n",
        "It uses CollectiveOps as the multi-worker all-reduce communication method used to keep variables in sync. A collective op is a single op in the TensorFlow graph which can automatically choose an all-reduce algorithm in the TensorFlow runtime according to hardware, network topology and tensor sizes.\n",
        "\n",
        "It also implements additional performance optimizations. For example, it includes a static optimization that converts multiple all-reductions on small tensors into fewer all-reductions on larger tensors. In addition, we are designing it to have a plugin architecture - so that in the future, you will be able to plugin algorithms that are better tuned for your hardware. Note that collective ops also implement other collective operations such as broadcast and all-gather."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yik7BTHvcDBX",
        "colab_type": "code",
        "outputId": "e8cee428-c79f-4e7b-fd5f-2b33d0b813b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/device:GPU:0',)\n",
            "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0',), communication = CollectiveCommunication.AUTO\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iC2HN3nyck7d",
        "colab_type": "text"
      },
      "source": [
        "MultiWorkerMirroredStrategy currently allows you to choose between two different implementations of collective ops. CollectiveCommunication.RING implements ring-based collectives using gRPC as the communication layer. CollectiveCommunication.NCCL uses Nvidia's NCCL to implement collectives. \n",
        "\n",
        "CollectiveCommunication.AUTO defers the choice to the runtime. \n",
        "\n",
        "The best choice of collective implementation depends upon the number and kind of GPUs, and the network interconnect in the cluster. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9R2U_5_ccal",
        "colab_type": "code",
        "outputId": "cff7fdbd-f0da-4d9b-fc11-0b81b008c1a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "multiworker_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(tf.distribute.experimental.CollectiveCommunication.NCCL)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Collective ops is not configured at program startup. Some performance features may not be enabled.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/device:GPU:0',)\n",
            "INFO:tensorflow:Single-worker MultiWorkerMirroredStrategy with local_devices = ('/device:GPU:0',), communication = CollectiveCommunication.NCCL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXx2sEL7eWN1",
        "colab_type": "text"
      },
      "source": [
        "# TPUStrategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YahIdJaegtx",
        "colab_type": "text"
      },
      "source": [
        "tf.distribute.experimental.TPUStrategy lets you run your TensorFlow training on Tensor Processing Units (TPUs). \n",
        "\n",
        "TPUs are Google's specialized ASICs designed to dramatically accelerate machine learning workloads."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3ZQqSrie2w-",
        "colab_type": "text"
      },
      "source": [
        "In terms of distributed training architecture, TPUStrategy is the same MirroredStrategy - it implements synchronous distributed training. \n",
        "\n",
        "TPUs provide their own implementation of efficient all-reduce and other collective operations across multiple TPU cores, which are used in TPUStrategy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXZJxsDjeUkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jv7ELDUleDVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.config.experimental_connect_to_cluster(cluster_resolver)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kypPLMHznPut",
        "colab_type": "code",
        "outputId": "2056c5ef-535e-4afd-f606-93a417b2bb9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "tf.tpu.experimental.initialize_tpu_system(cluster_resolver)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system 10.103.142.26:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system 10.103.142.26:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.103.142.26:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.103.142.26:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.tpu.topology.Topology at 0x7f8b873b7e48>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-BOVX88ndE-",
        "colab_type": "text"
      },
      "source": [
        "The TPUClusterResolver instance helps locate the TPUs. In Colab, you don't need to specify any arguments to it.\n",
        "\n",
        "If you want to use this for Cloud TPUs:\n",
        "\n",
        "- You must specify the name of your TPU resource in the tpu argument.\n",
        "- You must initialize the tpu system explicitly at the start of the program. This is required before TPUs can be used for computation. Initializing the tpu system also wipes out the TPU memory, so it's important to complete this step first in order to avoid losing state.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW6OepUmnljJ",
        "colab_type": "text"
      },
      "source": [
        "## ParameterServerStrategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX6G8i6WnsPr",
        "colab_type": "text"
      },
      "source": [
        "tf.distribute.experimental.ParameterServerStrategy supports parameter servers training on multiple machines. \n",
        "\n",
        "In this setup, some machines are designated as workers and some as parameter servers. Each variable of the model is placed on one parameter server. Computation is replicated across all GPUs of all the workers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvwEzVLYnVR9",
        "colab_type": "code",
        "outputId": "590af3fe-5996-4586-862c-6f4680ffd446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "source": [
        "ps_strategy = tf.distribute.experimental.ParameterServerStrategy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-3345f5629019>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mps_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameterServerStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/distribute/parameter_server_strategy.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cluster_resolver)\u001b[0m\n\u001b[1;32m    111\u001b[0m       \u001b[0mcluster_resolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTFConfigClusterResolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cluster spec must be non-empty in `cluster_resolver`.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m     extended = ParameterServerStrategyExtended(\n\u001b[1;32m    115\u001b[0m         self, cluster_resolver=cluster_resolver)\n",
            "\u001b[0;31mValueError\u001b[0m: Cluster spec must be non-empty in `cluster_resolver`."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_TJ3Jyd0MS0",
        "colab_type": "text"
      },
      "source": [
        "# One Device Strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyf4vJfH0NgX",
        "colab_type": "text"
      },
      "source": [
        "tf.distribute.OneDeviceStrategy runs on a single device. This strategy will place any variables created in its scope on the specified device. \n",
        "\n",
        "Input distributed through this strategy will be prefetched to the specified device. Moreover, any functions called via strategy.run will also be placed on the specified device.\n",
        "\n",
        "You can use this strategy to test your code before switching to other strategies which actually distributes to multiple devices/machines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KtTalW_0NBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "strategy = tf.distribute.OneDeviceStrategy(device=\"gpu:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prddayet0jmi",
        "colab_type": "text"
      },
      "source": [
        "# Using with tf.Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTXOBGXc1Ror",
        "colab_type": "text"
      },
      "source": [
        "We've integrated tf.distribute.Strategy into tf.keras which is TensorFlow's implementation of the Keras API specification. tf.keras is a high-level API to build and train models. By integrating into tf.keras backend, we've made it seamless for you to distribute your training written in the Keras training framework.\n",
        "\n",
        "Here's what you need to change in your code:\n",
        "\n",
        "- Create an instance of the appropriate tf.distribute.Strategy\n",
        "- Move the creation and compiling of Keras model inside strategy.scope.\n",
        "\n",
        "We support all types of Keras models - sequential, functional and subclassed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wBQsUxi0cYq",
        "colab_type": "code",
        "outputId": "7768e74d-fb64-4343-c080-8a0d77d3e633",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "mirrored_stratgy = tf.distribute.MirroredStrategy()\n",
        "with mirrored_stratgy.scope():\n",
        "    model = tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=(1, ))])\n",
        "    model.compile(loss='mse', optimizer='sgd')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVrKlSpR1s3H",
        "colab_type": "text"
      },
      "source": [
        "In this example we used MirroredStrategy so we can run this on a machine with multiple GPUs. strategy.scope() indicated which parts of the code to run distributed. \n",
        "\n",
        "Creating a model inside this scope allows us to create mirrored variables instead of regular variables. \n",
        "\n",
        "Compiling under the scope allows us to know that the user intends to train this model using this strategy. \n",
        "\n",
        "Once this is set up, you can fit your model like you would normally. \n",
        "\n",
        "MirroredStrategy takes care of replicating the model's training on the available GPUs, aggregating gradients, and more./"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1S1TBzH1nSO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(([1.], [1.])).repeat(100).batch(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXV2YIgK2Aew",
        "colab_type": "code",
        "outputId": "518c6cec-59b2-43b1-a587-eb82876ee69e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "model.fit(dataset, epochs=2)\n",
        "model.evaluate(dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 10 steps\n",
            "Epoch 1/2\n",
            "10/10 [==============================] - 4s 406ms/step - loss: 4.9371\n",
            "Epoch 2/2\n",
            "10/10 [==============================] - 0s 4ms/step - loss: 2.1822\n",
            "10/10 [==============================] - 0s 12ms/step - loss: 1.3552\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.3552119731903076"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xZJZRql2VjH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7KIpUkM2Hku",
        "colab_type": "code",
        "outputId": "f18bbebf-c498-4600-b5bd-c449dabece10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "inputs, targets = np.ones((100, 1)), np.ones((100, 1))\n",
        "history = model.fit(inputs, targets, epochs=2, batch_size=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 100 samples\n",
            "Epoch 1/2\n",
            "100/100 [==============================] - 1s 12ms/sample - loss: 0.9645\n",
            "Epoch 2/2\n",
            "100/100 [==============================] - 0s 249us/sample - loss: 0.4263\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0mj5qLiZ2gti",
        "colab_type": "text"
      },
      "source": [
        "In both cases (dataset or numpy), each batch of the given input is divided equally among the multiple replicas. \n",
        "\n",
        "For instance, if using MirroredStrategy with 2 GPUs, each batch of size 10 will get divided among the 2 GPUs, with each receiving 5 input examples in each step. \n",
        "\n",
        "Each epoch will then train faster as you add more GPUs. \n",
        "\n",
        "Typically, you would want to increase your batch size as you add more accelerators so as to make effective use of the extra computing power. You will also need to re-tune your learning rate, depending on the model. \n",
        "\n",
        "You can use strategy.num_replicas_in_sync to get the number of replicas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvBxqlwX2UYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCHES_SIZE_PER_REPLICA = 5\n",
        "global_batch_size = (BATCHES_SIZE_PER_REPLICA * mirrored_stratgy.num_replicas_in_sync)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAD4AMTL5CKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(([1.], [1.])).repeat(100)\n",
        "dataset = dataset.batch(global_batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wTrhDIu5P7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LEARNING_RATES_BY_BATCH_SIZE = {5: 0.1, 10: 0.15}\n",
        "learning_rate = LEARNING_RATES_BY_BATCH_SIZE[global_batch_size]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCrHSX8q53Sz",
        "colab_type": "text"
      },
      "source": [
        "# Support Currently "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55k2r-FP556m",
        "colab_type": "text"
      },
      "source": [
        "In TF 2.0 release, MirroredStrategy, TPUStrategy, CentralStorageStrategy and MultiWorkerMirroredStrategy are supported in Keras. \n",
        "\n",
        "Except MirroredStrategy, others are currently experimental and are subject to change. Support for other strategies will be coming soon. The API and how to use will be exactly the same as above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ahq7cTx7KCd",
        "colab_type": "text"
      },
      "source": [
        "# Simple Example "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-H-ZPS-5Sv4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "tfds.disable_progress_bar()\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Er5OJR497Q01",
        "colab_type": "code",
        "outputId": "a05b21ca-4ccd-4cc3-9215-3207615832a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "datasets, info = tfds.load(name=\"mnist\", with_info=True, as_supervised=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist (11.06 MiB) to /root/tensorflow_datasets/mnist/3.0.0...\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead set\n",
            "data_dir=gs://tfds-data/datasets.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abuSD4958s6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_train, mnist_test = datasets['train'], datasets['test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0u5rjNZ7fuC",
        "colab_type": "text"
      },
      "source": [
        "Create a MirroredStrategy object. This will handle distribution, and provides a context manager (tf.distribute.MirroredStrategy.scope) to build your model inside."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3c6qpEX7YVx",
        "colab_type": "code",
        "outputId": "538fa143-22ad-44d7-f1ca-9e5781430cfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "strategy = tf.distribute.MirroredStrategy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfBAZxYW7lFc",
        "colab_type": "code",
        "outputId": "54fe104d-8742-482d-d69a-30ed68a30bc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of devices: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JJ95mgv7r-I",
        "colab_type": "text"
      },
      "source": [
        "When training a model with multiple GPUs, you can use the extra computing power effectively by increasing the batch size. \n",
        "\n",
        "In general, use the largest batch size that fits the GPU memory, and tune the learning rate accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIqFjt5z7oMP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_train_examples = info.splits['train'].num_examples\n",
        "num_test_examples = info.splits['test'].num_examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDYd00Sz75Nr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 10000\n",
        "BUFFER_SIZE_PER_REPLICA = 64\n",
        "BATCH_SIZE = BATCHES_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiyTutyF8Fhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scale(image, label):\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image /= 255\n",
        "\n",
        "    return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAbihB5b8Sbx",
        "colab_type": "text"
      },
      "source": [
        "Apply this function to the training and test data, shuffle the training data, and batch it for training. \n",
        "\n",
        "Notice we are also keeping an in-memory cache of the training data to improve performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsdJf8K98PE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = mnist_train.map(scale).cache().shuffle(BATCH_SIZE).batch(BATCH_SIZE)\n",
        "eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5m0Cucn88xni",
        "colab_type": "text"
      },
      "source": [
        "Create and compile the Keras model in the context of strategy.scope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvBOh1MU8xZL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "    model = tf.keras.Sequential([\n",
        "                                 tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
        "                                 tf.keras.layers.MaxPooling2D(),\n",
        "                                 tf.keras.layers.Flatten(),\n",
        "                                 tf.keras.layers.Dense(64, activation='relu'),\n",
        "                                 tf.keras.layers.Dense(10)\n",
        "    ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ55PXs38llZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), optimizer=tf.keras.optimizers.Adam(), metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0ZTdhxv9eEx",
        "colab_type": "text"
      },
      "source": [
        "The callbacks used here are:\n",
        "\n",
        "- TensorBoard: This callback writes a log for TensorBoard which allows you to visualize the graphs.\n",
        "- Model Checkpoint: This callback saves the model after every epoch.\n",
        "- Learning Rate Scheduler: Using this callback, you can schedule the learning rate to change after every epoch/batch.\n",
        "\n",
        "For illustrative purposes, add a print callback to display the learning rate in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e67cyvLW9Z_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = '/training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZIZKyBX9wYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decay(epoch):\n",
        "    if epoch < 3:\n",
        "       return 1e-3\n",
        "    elif epoch >=3 and epoch <= 7:\n",
        "        return 1e-4\n",
        "    else:\n",
        "        return 1e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7THgZhK96uD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PrintLR(tf.keras.callbacks.Callback):\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print('\\nLearning rate for epoch {} is {}'.format(epoch + 1, model.optimizer.lr.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy42-pOf-PaK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
        "                                       save_weights_only=True),\n",
        "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
        "    PrintLR()\n",
        "]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQ9__7l7-U6D",
        "colab_type": "text"
      },
      "source": [
        "Now, train the model in the usualway, calling fit on the model and passing in the dataset created at the beginning of the tutorial. This step is the same whether you are distributing the training or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7MU2wVx-Sf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit(train_dataset, epochs=12, callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-grE9SnBebU",
        "colab_type": "text"
      },
      "source": [
        "# Custom Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piScUZUjBohd",
        "colab_type": "text"
      },
      "source": [
        "This tutorial demonstrates how to use tf.distribute.Strategy with custom training loops. \n",
        "\n",
        "We will train a simple CNN model on the fashion MNIST dataset. The fashion MNIST dataset contains 60000 train images of size 28 x 28 and 10000 test images of size 28 x 28.\n",
        "\n",
        "We are using custom training loops to train our model because they give us flexibility and a greater control on training. Moreover, it is easier to debug the model and the training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKF3Lotn-ZHT",
        "colab_type": "code",
        "outputId": "8cc6fb04-56c3-47f0-9caa-ca5849cfc316",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9-yu1-7B2Y1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = tf.expand_dims(train_images, axis=-1)\n",
        "test_images = tf.expand_dims(test_images, axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkjJCPEGCIlN",
        "colab_type": "code",
        "outputId": "4fc6d689-29ac-401f-bc56-86837302263b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(train_images.shape, test_images.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1) (10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1MJxtC1CMG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_images = train_images / np.float32(255)\n",
        "test_images = test_images / np.float32(255)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiwZyQ87C8Hd",
        "colab_type": "text"
      },
      "source": [
        "Create a strategy to distribute the variables and the graph\n",
        "\n",
        "How does tf.distribute.MirroredStrategy strategy work?\n",
        "\n",
        "All the variables and the model graph is replicated on the replicas.\n",
        "\n",
        "Input is evenly distributed across the replicas.\n",
        "\n",
        "Each replica calculates the loss and gradients for the input it received.\n",
        "\n",
        "The gradients are synced across all the replicas by summing them.\n",
        "\n",
        "After the sync, the same update is made to the copies of the variables on each replica.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1uPKQ2KDp2t",
        "colab_type": "code",
        "outputId": "c231555c-44fe-4565-a1c9-f46759d9cc61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "strategy = tf.distribute.MirroredStrategy()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiB9McKUC3tF",
        "colab_type": "code",
        "outputId": "9353a4d8-83c3-4410-aafa-312db5f17424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of devices: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GX_lwlqaHJXQ",
        "colab_type": "text"
      },
      "source": [
        "## Setup input pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQQ5sW1VHM5z",
        "colab_type": "text"
      },
      "source": [
        "Export the graph and the variables to the platform-agnostic SavedModel format. \n",
        "\n",
        "After your model is saved, you can load it with or without the scope."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBFUzAuiDtBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(train_images)\n",
        "BATCHES_SIZE_PER_REPLICA = 64\n",
        "GLOBAL_BATCH_SIZE = BATCHES_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMvv0ODrHb0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coNOBxgkHj-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels)).shuffle(BUFFER_SIZE).batch(GLOBAL_BATCH_SIZE)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels)).batch(GLOBAL_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYe-aVznIAFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dist_dataset = strategy.experimental_distribute_dataset(train_dataset)\n",
        "test_dist_dataset  = strategy.experimental_distribute_dataset(test_dataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ0lUUyrIOuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    model = tf.keras.Sequential([\n",
        "                                 tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "                                 tf.keras.layers.MaxPooling2D(),\n",
        "                                 tf.keras.layers.Conv2D(64, 3, activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.Dense(10)\n",
        "    ])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqbaV_q5InAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpCr3R_B9Y_A",
        "colab_type": "text"
      },
      "source": [
        "# Define the loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-_4eImu9aBX",
        "colab_type": "text"
      },
      "source": [
        "Normally, on a single machine with 1 GPU/CPU, loss is divided by the number of examples in the batch of input.\n",
        "\n",
        "So, how should the loss be calculated when using a tf.distribute.Strategy?\n",
        "\n",
        "- For an example, let's say you have 4 GPU's and a batch size of 64. One batch of input is distributed across the replicas (4 GPUs), each replica getting an input of size 16.\n",
        "\n",
        "- The model on each replica does a forward pass with its respective input and calculates the loss. Now, instead of dividing the loss by the number of examples in its respective input (BATCH_SIZE_PER_REPLICA = 16), the loss should be divided by the GLOBAL_BATCH_SIZE (64).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5qwhzYJ9msB",
        "colab_type": "text"
      },
      "source": [
        "Why do this?\n",
        "\n",
        "- This needs to be done because after the gradients are calculated on each replica, they are synced across the replicas by summing them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFMJQjwk90bR",
        "colab_type": "text"
      },
      "source": [
        "How to do this in TensorFlow?\n",
        "\n",
        "- If you're writing a custom training loop, as in this tutorial, you should sum the per example losses and divide the sum by the GLOBAL_BATCH_SIZE: scale_loss = tf.reduce_sum(loss) * (1. / GLOBAL_BATCH_SIZE) or you can use tf.nn.compute_average_loss which takes the per example loss, optional sample weights, and GLOBAL_BATCH_SIZE as arguments and returns the scaled loss.\n",
        "\n",
        "- If you are using regularization losses in your model then you need to scale the loss value by number of replicas. You can do this by using the tf.nn.scale_regularization_loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt3zM0t-98l7",
        "colab_type": "text"
      },
      "source": [
        "- Using tf.reduce_mean is not recommended. Doing so divides the loss by actual per replica batch size which may vary step to step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xR-Ww0tQ987l",
        "colab_type": "text"
      },
      "source": [
        "- This reduction and scaling is done automatically in keras model.compile and model.fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmRQSpbV-H9O",
        "colab_type": "text"
      },
      "source": [
        "- If using tf.keras.losses classes (as in the example below), the loss reduction needs to be explicitly specified to be one of NONE or SUM. AUTO and SUM_OVER_BATCH_SIZE are disallowed when used with tf.distribute.Strategy. AUTO is disallowed because the user should explicitly think about what reduction they want to make sure it is correct in the distributed case. SUM_OVER_BATCH_SIZE is disallowed because currently it would only divide by per replica batch size, and leave the dividing by number of replicas to the user, which might be easy to miss. So instead we ask the user do the reduction themselves explicitly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVIVtlcDIxdq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "    # Set reduction to `none` so we can do the reduction afterwards and divide by\n",
        "    # global batch size.\n",
        "\n",
        "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "    def compute_loss(labels, predictions):\n",
        "        per_example_loss = loss_object(labels, predictions)\n",
        "        return tf.nn.compute_average_loss(per_example_loss, global_batch_size=GLOBAL_BATCH_SIZE)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sryxd20x-uek",
        "colab_type": "text"
      },
      "source": [
        "# Define the metrics to track loss and accuracy\n",
        "\n",
        "These metrics track the test loss and training and test accuracy. You can use .result() to get the accumulated statistics at any time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UxkMYId6-qUg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "    test_loss = tf.keras.metrics.Mean(name=\"test_loss\")\n",
        "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")\n",
        "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"test_accuracy\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVraxaK5_mto",
        "colab_type": "text"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LeVXRj4G_hjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "    model = create_model()\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbxTwTaJACWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  def train_step(inputs):\n",
        "    images, labels = inputs\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = model(images, training=True)\n",
        "      loss = compute_loss(labels, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_accuracy.update_state(labels, predictions)\n",
        "    return loss \n",
        "\n",
        "  def test_step(inputs):\n",
        "    images, labels = inputs\n",
        "\n",
        "    predictions = model(images, training=False)\n",
        "    t_loss = loss_object(labels, predictions)\n",
        "\n",
        "    test_loss.update_state(t_loss)\n",
        "    test_accuracy.update_state(labels, predictions)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "by1lzZDwBHQY",
        "colab_type": "code",
        "outputId": "547d9bcb-a303-45b5-f0ab-c208aa4f7991",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        }
      },
      "source": [
        "with strategy.scope():\n",
        "  # `run` replicates the provided computation and runs it\n",
        "  # with the distributed input.\n",
        "  @tf.function\n",
        "  def distributed_train_step(dataset_inputs):\n",
        "    per_replica_losses = strategy.experimental_run_v2(train_step, args=(dataset_inputs,))\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "                           axis=None)\n",
        " \n",
        "  @tf.function\n",
        "  def distributed_test_step(dataset_inputs):\n",
        "    return strategy.experimental_run_v2(test_step, args=(dataset_inputs,))\n",
        "\n",
        "  for epoch in range(EPOCHS):\n",
        "    # TRAIN LOOP\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "    for x in train_dist_dataset:\n",
        "      total_loss += distributed_train_step(x)\n",
        "      num_batches += 1\n",
        "    train_loss = total_loss / num_batches\n",
        "\n",
        "    # TEST LOOP\n",
        "    for x in test_dist_dataset:\n",
        "      distributed_test_step(x)\n",
        "\n",
        "    if epoch % 2 == 0:\n",
        "      checkpoint.save(checkpoint_prefix)\n",
        "\n",
        "    template = (\"Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, \"\n",
        "                \"Test Accuracy: {}\")\n",
        "    print (template.format(epoch+1, train_loss,\n",
        "                           train_accuracy.result()*100, test_loss.result(),\n",
        "                           test_accuracy.result()*100))\n",
        "\n",
        "    test_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_accuracy.reset_states()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 1, Loss: 0.5085545778274536, Accuracy: 81.89833068847656, Test Loss: 0.3894329369068146, Test Accuracy: 85.86000061035156\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "Epoch 2, Loss: 0.33334583044052124, Accuracy: 88.00833129882812, Test Loss: 0.3213692009449005, Test Accuracy: 88.47000122070312\n",
            "Epoch 3, Loss: 0.29127976298332214, Accuracy: 89.38333129882812, Test Loss: 0.31748753786087036, Test Accuracy: 88.73999786376953\n",
            "Epoch 4, Loss: 0.26079073548316956, Accuracy: 90.40666961669922, Test Loss: 0.28609830141067505, Test Accuracy: 89.4800033569336\n",
            "Epoch 5, Loss: 0.2375706285238266, Accuracy: 91.28666687011719, Test Loss: 0.2764546275138855, Test Accuracy: 89.80000305175781\n",
            "Epoch 6, Loss: 0.21696174144744873, Accuracy: 91.90499877929688, Test Loss: 0.26529788970947266, Test Accuracy: 90.3499984741211\n",
            "Epoch 7, Loss: 0.19971436262130737, Accuracy: 92.67500305175781, Test Loss: 0.2648813724517822, Test Accuracy: 90.2300033569336\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bigkQo0aFXqG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
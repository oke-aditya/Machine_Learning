{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Custom_layers_tensorflow.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBN-aKbm7G7R",
        "colab_type": "code",
        "outputId": "29c767aa-20cc-44c7-d40c-7bfcc002defa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ8XOYDK7wjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXAu80Ra7tUl",
        "colab_type": "code",
        "outputId": "dce992b8-c847-48d8-f962-48e405ef0049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hi0mI3gG79ej",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow provides both a set of many common layers as a well as easy ways for you to write your own application-specific layers either from scratch or as the composition of existing layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R5mI0Ue8Hz7",
        "colab_type": "text"
      },
      "source": [
        "Implementing custom layers\n",
        "\n",
        "The best way to implement your own layer is extending the tf.keras.Layer class and implementing: * __init__ , where you can do all input-independent initialization * \n",
        "build, where you know the shapes of the input tensors and can do the rest of the initialization * call, where you do the forward computation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlFHbbrC9ylN",
        "colab_type": "text"
      },
      "source": [
        "# Creating class for layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qjPU4sS7y4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class my_dense_layer(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_outputs):\n",
        "        super().__init__()\n",
        "        self.num_outputs = num_outputs\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        self.kernel = self.add_weight(\"kernel\", shape=[int(input_shape[-1]), self.num_outputs])\n",
        "    \n",
        "    def call(self, input):\n",
        "        return tf.matmul(input, self.kernel)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfyj_jDr81od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer = my_dense_layer(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09JpoY1T83pT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "op = layer(tf.zeros([10, 5]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8ByZ0k-8-tM",
        "colab_type": "code",
        "outputId": "d6af95a1-67d1-4ef2-c022-1829e63241b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print([var.name for var in layer.trainable_variables])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['my_dense_layer/kernel:0']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy6tXmz39Msf",
        "colab_type": "text"
      },
      "source": [
        "Overall code is easier to read and maintain if it uses standard layers whenever possible, as other readers will be familiar with the behavior of standard layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSoMe2Ia9vJ7",
        "colab_type": "text"
      },
      "source": [
        "# Composing Layers to create Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zox-aJCcBku2",
        "colab_type": "text"
      },
      "source": [
        "Many interesting layer-like things in machine learning models are implemented by composing existing layers. For example, each residual block in a resnet is a composition of convolutions, batch normalizations, and a shortcut. Layers can be nested inside other layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5G9u0HCBoGq",
        "colab_type": "text"
      },
      "source": [
        "Typically you inherit from keras.Model when you need the model methods like: Model.fit,Model.evaluate, and Model.save "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTCYnXlvCQ4D",
        "colab_type": "text"
      },
      "source": [
        "One other feature provided by keras.Model (instead of keras.layers.Layer) is that in addition to tracking variables, a keras.Model also tracks its internal layers, making them easier to inspect."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzW3zsoJ9Iw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class resnet_block(tf.keras.Model):\n",
        "    def __init__(self, kernel_size, filters):\n",
        "        super().__init__()\n",
        "        filters1, filters2, filters3 = filters\n",
        "\n",
        "        self.conv2a = tf.keras.layers.Conv2D(filters1, (1, 1))\n",
        "        self.bn2a = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "        self.conv2b = tf.keras.layers.Conv2D(filters2, kernel_size, padding='same')\n",
        "        self.bn2b = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "        self.conv2c = tf.keras.layers.Conv2D(filters3, kernel_size, padding='same')\n",
        "        self.bn2c = tf.keras.layers.BatchNormalization()\n",
        "\n",
        "    \n",
        "    def call(self, input_tensor, training=False):\n",
        "        x = self.conv2a(input_tensor)\n",
        "        x = self.bn2a(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "\n",
        "        x = self.conv2b(x)\n",
        "        x = self.bn2b(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "\n",
        "        x = self.conv2c(x)\n",
        "        x = self.bn2c(x, training=training)\n",
        "        \n",
        "        x += input_tensor\n",
        "        return tf.nn.relu(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8DCh4wja2sJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "block = resnet_block(1, [1,2,3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU3Qu7o_a_Up",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_ = block(tf.zeros([1, 2, 3, 3]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmAP408NbCmP",
        "colab_type": "code",
        "outputId": "4f45c578-9779-40c8-caa7-8d36b73a76b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        }
      },
      "source": [
        "block.layers"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa98f4e9e48>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fa98f4e7a58>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa98f4e7b38>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fa98f4e74a8>,\n",
              " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7fa98f4e7518>,\n",
              " <tensorflow.python.keras.layers.normalization_v2.BatchNormalization at 0x7fa98f4e7f98>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdgV_2NpbPkJ",
        "colab_type": "code",
        "outputId": "00c2a3ce-8366-4b0c-a9da-a1eca98ad7b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "block.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"resnet_block\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              multiple                  4         \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo multiple                  4         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            multiple                  4         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch multiple                  8         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            multiple                  9         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch multiple                  12        \n",
            "=================================================================\n",
            "Total params: 41\n",
            "Trainable params: 29\n",
            "Non-trainable params: 12\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0xbRrr6beQu",
        "colab_type": "text"
      },
      "source": [
        "# Writing Custom layers and Models in Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmWyXPo0bTl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBIUdBS-3dJN",
        "colab_type": "text"
      },
      "source": [
        "## The Layer class\n",
        "\n",
        "The main data structure you'll work with is the Layer. A layer encapsulates both a state (the layer's \"weights\") and a transformation from inputs to outputs (a \"call\", the layer's forward pass).\n",
        "\n",
        "Here's a densely-connected layer. It has a state: the variables w and b."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWT16OpodbVs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DMxOcJu4JzQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear(layers.Layer):\n",
        "\n",
        "    def __init__(self, units=32, input_dim=32):\n",
        "        super().__init__()\n",
        "        w_init = tf.random_normal_initializer()\n",
        "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units), dtype='float32'), trainable=True)\n",
        "        b_init = tf.zeros_initializer()\n",
        "        self.b = tf.Variable(initial_value=b_init(shape=(units, ), dtype=tf.float32), trainable=True)\n",
        "\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSsvduDN5HSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.ones((2, 2))\n",
        "linear_layer = Linear(4, 2)\n",
        "y = linear_layer(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVXTP7Tm5OZU",
        "colab_type": "code",
        "outputId": "b9c9b9d5-fb3b-49a2-ee44-3906e5231f66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.0302961   0.03911199  0.06768572 -0.04566586]\n",
            " [ 0.0302961   0.03911199  0.06768572 -0.04566586]], shape=(2, 4), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWluQEGe5lNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear(layers.Layer):\n",
        "    def __init__(self, units=32, input_dim=32):\n",
        "        super(Linear, self).__init__()\n",
        "        self.w = self.add_weight(shape=(input_dim, units), initializer='random_normal', trainable=True)\n",
        "        self.b = self.add_weight(shape=(units, ), initializer='zeros', trainable=True)\n",
        "\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        return tf.matmul(inputs, self.w) + self.b\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZATD4PSW6VFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.ones((2,2))\n",
        "linear_layer = Linear(4, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOnwLZkY6lMu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = linear_layer(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBVLtjzw6sP1",
        "colab_type": "code",
        "outputId": "6996f2ec-864c-4e71-9d43-0797e0b1b446",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[ 0.02960566  0.08698709  0.11366069 -0.01657881]\n",
            " [ 0.02960566  0.08698709  0.11366069 -0.01657881]], shape=(2, 4), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QoeKAme7wWX",
        "colab_type": "text"
      },
      "source": [
        "# Building Models\n",
        "- The Model class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8o51hK77unE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "In general, you will use the Layer class to define inner computation blocks, and will use the Model class to define the outer model -- the object you will train.\n",
        "\n",
        "For instance, in a ResNet50 model, you would have several ResNet blocks subclassing Layer, and a single Model encompassing the entire ResNet50 network.\n",
        "\n",
        "The Model class has the same API as Layer, with the following differences:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5jIZcbt8Aei",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "- It exposes built-in training, evaluation, and prediction loops (model.fit(), model.evaluate(), model.predict()).\n",
        "- It exposes the list of its inner layers, via the model.layers property.\n",
        "- It exposes saving and serialization APIs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CoeERvsb8F_p",
        "colab_type": "text"
      },
      "source": [
        "Effectively, the \"Layer\" class corresponds to what we refer to in the literature as a \"layer\" (as in \"convolution layer\" or \"recurrent layer\") or as a \"block\" (as in \"ResNet block\" or \"Inception block\").\n",
        "\n",
        "Meanwhile, the \"Model\" class corresponds to what is referred to in the literature as a \"model\" (as in \"deep learning model\") or as a \"network\" (as in \"deep neural network\").\n",
        "\n",
        "For instance, we could take our mini-resnet example above, and use it to build a Model that we could train with fit(), and that we could save with save_weights:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lokCMnp66tGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.block_1 = ResNetBlock()\n",
        "        self.block_2 = ResNetBlock()\n",
        "        self.global_pool = layers.GlobalAveragePooling2D()\n",
        "        self.classifier = Dense(num_classes)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        x = self.block_1(inputs)\n",
        "        x = self.block_2(x)\n",
        "        x = self.global_pool(x)\n",
        "        return self.classifier(x)\n",
        "    \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ehcb5zC9C_O",
        "colab_type": "text"
      },
      "source": [
        "# End to end Example of doing VAE using custom class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1XinWVp9qM7",
        "colab_type": "text"
      },
      "source": [
        "Here's what you've learned so far:\n",
        "\n",
        "    A Layer encapsulate a state (created in __init__ or build) and some computation (in call).\n",
        "    Layers can be recursively nested to create new, bigger computation blocks.\n",
        "    Layers can create and track losses (typically regularization losses).\n",
        "    The outer container, the thing you want to train, is a Model. A Model is just like a Layer, but with added training and serialization utilities.\n",
        "\n",
        "Let's put all of these things together into an end-to-end example: we're going to implement a Variational AutoEncoder (VAE). We'll train it on MNIST digits.\n",
        "\n",
        "Our VAE will be a subclass of Model, built as a nested composition of layers that subclass Layer. It will feature a regularization loss (KL divergence)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBNAO2qnLhu4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJwJkEdh8v-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQz61_DX-Uek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(layers.Layer):\n",
        "    \"\"\"Maps MNIST digits to a triplet (z_mean, z_log_var, z).\"\"\"\n",
        "\n",
        "    def __init__(self, latent_dim=32, intermediate_dim=64, name='encoder', **kwargs):\n",
        "        super(Encoder, self).__init__(name=name, **kwargs)\n",
        "        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
        "        self.dense_mean = layers.Dense(latent_dim)\n",
        "        self.dense_log_var = layers.Dense(latent_dim)\n",
        "        self.sampling = sampling()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense_proj(inputs)\n",
        "        z_mean = self.dense_mean(x)\n",
        "        z_log_var = self.dense_log_var(x)\n",
        "        z = self.sampling((z_mean, z_log_var))\n",
        "        return z_mean, z_log_var, z\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57mrAhO7LnOX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ec = Encoder(32, 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xsk_hypDCHO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(layers.Layer):\n",
        "    \"\"\"Converts z, the encoded digit vector, back into a readable digit.\"\"\"\n",
        "    def __init__(self, original_dim, intermediate_dim=64, name='decoder', **kwargs):\n",
        "        super(Decoder, self).__init__(name=name, **kwargs)\n",
        "        self.dense_proj = layers.Dense(intermediate_dim, activation='relu')\n",
        "        self.dense_output = layers.Dense(original_dim, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.dense_proj(inputs)\n",
        "        return self.dense_output(x)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIPzfxUQLrdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dc = Decoder(32, 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-fdXhbTEMHt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class vae(tf.keras.Model):\n",
        "    \"\"\"Combines the encoder and decoder into an end-to-end model for training.\"\"\"\n",
        "    def __init__(self, orignal_dim, intermediate_dim=64, latent_dim=32, name='autoencoder', **kwargs):\n",
        "        super().__init__()\n",
        "        self.orignal_dims = orignal_dim\n",
        "        self.encoder = Encoder(latent_dim=latent_dim, intermediate_dim=intermediate_dim)\n",
        "        self.decoder = Decoder(orignal_dim, intermediate_dim=intermediate_dim)\n",
        "    \n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var, z = self.encoder(inputs)\n",
        "        reconstructed = self.decoder(z)\n",
        "        kl_loss = - 0.5 * tf.reduce_mean(\n",
        "        z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "        self.add_loss(kl_loss)\n",
        "        return reconstructed\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8B41zKlFkV6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original_dim = 784\n",
        "va = vae(original_dim, 64, 32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAOGueV_F3SK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "loss_metric = tf.keras.metrics.Mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qf9If17Gnnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, _), _ = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(60000, x_train.shape[1] * x_train.shape[2]).astype('float32') / 255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HO3mlyCSMjbF",
        "colab_type": "code",
        "outputId": "e7185cd0-7762-454f-db46-abafbe77e280",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(x_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9L0mbhZGxWs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices(x_train)\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-QqAmT-G_sh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ej_VxeWHH00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    print('Start of epoch %d' % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, x_batch_train in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            reconstructed = vae(x_batch_train)\n",
        "            # Compute reconstruction loss\n",
        "            loss = mse_loss_fn(x_batch_train, reconstructed)\n",
        "            loss += sum(vae.losses)  # Add KLD regularization loss\n",
        "\n",
        "    grads = tape.gradient(loss, vae.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "\n",
        "    loss_metric(loss)\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print('step %s: mean loss = %s' % (step, loss_metric.result()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGFb2sOlJJxb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae_2 = vae(784, 64, 32)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
        "\n",
        "vae_2.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())\n",
        "vae_2.fit(x_train, x_train, epochs=3, batch_size=64)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ik3YnomHKPw9",
        "colab_type": "text"
      },
      "source": [
        "# Same stuff can be done with Functional API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGBun1l4KUDY",
        "colab_type": "text"
      },
      "source": [
        "You can also build models using the Functional API. Importantly, choosing one style or another does not prevent you from leveraging components written in the other style: you can always mix-and-match.\n",
        "\n",
        "For instance, the Functional API example below reuses the same Sampling layer we defined in the example above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q3NR1WdJen0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "orignal_dim = 784\n",
        "intermediate_dim = 64\n",
        "latent_dim = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LalWh_HSXU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class sampling(layers.Layer):\n",
        "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
        "\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_log_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gheD_6uVR2gC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "original_inputs = tf.keras.Input(shape=(orignal_dim, ), name=\"encoder_inputs\")\n",
        "x = layers.Dense(intermediate_dim, activation='relu') (original_inputs)\n",
        "z_mean = layers.Dense(latent_dim, name=\"z_mean\") (x)\n",
        "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\") (x)\n",
        "z = sampling()((z_mean, z_log_var))\n",
        "encoder = tf.keras.Model(inputs=original_inputs, outputs=z, name=\"encoder\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LakRYQBwSjsn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "latent_inputs = tf.keras.Input(shape=(latent_dim, ), name=\"z_sampling\")\n",
        "x = layers.Dense(intermediate_dim, activation='relu') (latent_inputs)\n",
        "outputs = layers.Dense(orignal_dim, activation='sigmoid') (x)\n",
        "decoder = tf.keras.Model(inputs=latent_inputs, outputs=outputs, name=\"decoder\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGVvb0IUTcyD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs = decoder(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGS6OI1rTs22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vae = tf.keras.Model(inputs=original_inputs, outputs=outputs, name='vae')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ic2R0deTzjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kl_loss = - 0.5 * tf.reduce_mean(z_log_var - tf.square(z_mean) - tf.exp(z_log_var) + 1)\n",
        "vae.add_loss(kl_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELEE_gDdT4hZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(lr=1e-3)\n",
        "vae.compile(optimizer, loss=tf.keras.losses.MeanSquaredError())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTP37srwUDe8",
        "colab_type": "code",
        "outputId": "1f68b48c-3285-4641-c23c-7237cf2ce3c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "source": [
        "history = vae.fit(x_train, x_train, epochs=3, batch_size=64, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/3\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.0675\n",
            "Epoch 2/3\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0675\n",
            "Epoch 3/3\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EuqnRllfUJyl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
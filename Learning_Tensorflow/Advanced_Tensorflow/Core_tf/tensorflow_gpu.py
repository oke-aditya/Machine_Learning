# -*- coding: utf-8 -*-
"""tensorflow_gpu

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1REzBNX1eDjb750VGR4Rc47LQv8z7rDBO
"""

!nvidia-smi

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf

import os, datetime
import tensorflow_datasets as tfds

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# To understand device logging and how things go within
# Do not use this line in production
tf.debugging.set_log_device_placement(True)

tf.config.experimental.list_physical_devices()
# XLA is compiler that speeds up the tf graph

"""# Getting Data"""

# try gcs is must for TPU
dataset, info = tfds.load(name='fashion_mnist', with_info=True, as_supervised=True, try_gcs=True, split=[ 'train', 'test'])

info

print(info.features)

print(info.features['label'].num_classes)

print(info.features['label'].names)

fm_train, fm_test = dataset[0], dataset[1]
fm_val = fm_test.take(3000)
fm_test = fm_test.skip(3000)

print(fm_train)

import matplotlib.pyplot as plt
import numpy as np

for fm_sample in fm_train.take(5):
    image, label = fm_sample[0], fm_sample[1]
    
    plt.imshow(image.numpy()[:, :, 0], cmap='gray')
    plt.axis("off")
    plt.show()
    print("Label : %s" %(label.numpy()))

"""# Augmenting and training"""

# For tensorboard
!mkdir /tmp/logs

def scale(image, label):
    image = tf.cast(image, tf.float32)
    image /= 255.0

    return image, label

def get_dataset(batch_size=64):
    train_dataset_scaled = fm_train.map(scale).shuffle(60000).batch(batch_size)
    test_dataset_scaled = fm_test.map(scale).batch(batch_size)
    val_dataset_scaled = fm_val.map(scale).batch(batch_size)

    return train_dataset_scaled, test_dataset_scaled, val_dataset_scaled

def create_model():
    return tf.keras.Sequential([
                                tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
                                tf.keras.layers.MaxPooling2D(),
                                tf.keras.layers.Flatten(),
                                tf.keras.layers.Dense(256, activation='relu'),
                                tf.keras.layers.Dropout(0.2),
                                tf.keras.layers.Dense(10, activation='softmax')
    ])

model = create_model()

model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['acc'])

logdir = os.path.join('/tmp/logs', datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)

train_dataset, test_dataset, val_dataset = get_dataset()

train_dataset.cache()
test_dataset.cache()

history = model.fit(train_dataset, epochs=5, steps_per_epoch=20, validation_data=val_dataset, callbacks=[tensorboard_callback])

print(tf.distribute.get_strategy())

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir /tmp/logs

! nvidia-smi

def create_model_deep():
    return tf.keras.Sequential([
                                tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
                                tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
                                # tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
                                tf.keras.layers.MaxPooling2D(),
                                tf.keras.layers.Flatten(),
                                tf.keras.layers.Dense(256, activation='relu'),
                                tf.keras.layers.Dropout(0.2),
                                tf.keras.layers.Dense(10, activation='softmax')
    ])

strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
    train_dataset, test_dataset, val_dataset = get_dataset()
    train_dataset.cache()
    test_dataset.cache()

    model_deep = create_model_deep()
    model_deep.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['acc'])
    
    logdir = os.path.join('/tmp/logs', datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)
    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath="best_model.hdf5", verbose=1, save_best_only=True)
    csv_logger = tf.keras.callbacks.CSVLogger('/tmp/history.log')

    history = model_deep.fit(train_dataset, epochs=10, steps_per_epoch=20, validation_data=val_dataset, callbacks=[tensorboard_callback, checkpointer, csv_logger])

strategy = tf.distribute.MirroredStrategy()
with strategy.scope():
    print(tf.distribute.get_strategy())

# strategy = tf.distribute.MirroredStrategy()

with tf.device("CPU:0"):
    train_dataset, test_dataset, val_dataset = get_dataset()
    train_dataset.cache()
    test_dataset.cache()

    model_deep = create_model_deep()
    model_deep.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['acc'])
    
    logdir = os.path.join('/tmp/logs', datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
    tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)
    checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath="best_model.hdf5", verbose=1, save_best_only=True)
    csv_logger = tf.keras.callbacks.CSVLogger('/tmp/history.log')

    history = model_deep.fit(train_dataset, epochs=10, steps_per_epoch=20, validation_data=val_dataset, callbacks=[tensorboard_callback, checkpointer, csv_logger])
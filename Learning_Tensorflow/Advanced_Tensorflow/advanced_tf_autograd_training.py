# -*- coding: utf-8 -*-
"""advanced_tf_autograd_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jL1o7JUpbEw8u2DHl7Z4Bjdn7gJHb6x3

# AutoGrad API in Tensorflow
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

import tensorflow as tf

"""# Gradient Tape

TensorFlow provides the tf.GradientTape API for automatic differentiation - computing the gradient of a computation with respect to its input variables. Tensorflow "records" all operations executed inside the context of a tf.GradientTape onto a "tape". 

Tensorflow then uses that tape and the gradients associated with each recorded operation to compute the gradients of a "recorded" computation using reverse mode differentiation.
"""

x = tf.ones((2, 2))

with tf.GradientTape() as tape:
    tape.watch(x)
    y = tf.reduce_sum(x)
    z = tf.multiply(y, y)

dz_dx = tape.gradient(z, x)

print(dz_dx)

"""You can also request gradients of the output with respect to intermediate values computed during a "recorded" tf.GradientTape context."""

x = tf.ones((2, 2))
with tf.GradientTape() as tape:
    tape.watch(x)
    y = tf.reduce_sum(x)
    z = tf.multiply(y, y)

dz_dy = tape.gradient(z, y)
print(dz_dy)

"""By default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected."""

x = tf.constant(3.0)
with tf.GradientTape(persistent=True) as tape:
    tape.watch(x)
    y = x * x
    z = y * y

dz_dx = tape.gradient(z, x)
dz_dy = tape.gradient(z, y)
dy_dx = tape.gradient(y, x)
print(dz_dx, dz_dy, dy_dx)

del tape

"""Recording control flow

Because tapes record operations as they are executed, Python control flow (using ifs and whiles for example) is naturally handled:
"""

def f(x, y):
    output = 1.0
    for i in range(y):
        if i > 1 and i < 5:
            output = tf.multiply(output, x)
    return output

def grad(x, y):
    with tf.GradientTape() as tape:
        tape.watch(x)
        out = f(x, y)
    
    return tape.gradient(out, x)

x = tf.convert_to_tensor(2.0)

x

print(grad(x, 6))

print(grad(x, 5))

print(grad(x, 4))

"""# Higher Order Gradients

Operations inside of the GradientTape context manager are recorded for automatic differentiation. If gradients are computed in that context, then the gradient computation is recorded as well. As a result, the exact same API works for higher-order gradients as well. For example:
"""

x = tf.Variable(1.0)

with tf.GradientTape() as tape1:
    with tf.GradientTape() as tape2:
        y = x * x * x
        dy_dx = tape2.gradient(y, x)
        d2y_dx2 = tape1.gradient(dy_dx, x)

print(dy_dx)
print(d2y_dx2)

"""# Using @tf.function

In TensorFlow 2.0, eager execution is turned on by default. The user interface is intuitive and flexible (running one-off operations is much easier and faster), but this can come at the expense of performance and deployability.

To get peak performance and to make your model deployable anywhere, use tf.function to make graphs out of your programs. Thanks to AutoGraph, a surprising amount of Python code just works with tf.function, but there are still pitfalls to be wary of.

The main takeaways and recommendations are:

    Don't rely on Python side effects like object mutation or list appends.
    tf.function works best with TensorFlow ops, rather than NumPy ops or Python primitives.
    When in doubt, use the ` for x in y idiom `.
"""

import traceback
import contextlib

# Some helper code to demonstrate the kinds of errors you might encounter.
@contextlib.contextmanager
def assert_raises(error_class):
  try:
    yield
  except error_class as e:
    print('Caught expected exception \n  {}:'.format(error_class))
    traceback.print_exc(limit=2)
  except Exception as e:
    raise e
  else:
    raise Exception('Expected {} to be raised but no error was raised!'.format(
        error_class))

"""## Basics

A tf.function you define is just like a core TensorFlow operation: You can execute it eagerly; you can use it in a graph; it has gradients; and so on.
"""

@tf.function
def add(a, b):
    return a + b

add(tf.ones([2, 2]), tf.ones([2, 2]))

v = tf.Variable(1.0)
with tf.GradientTape() as tape:
    result = add(v, 10)

tape.gradient(result, v)

"""You can use functions inside functions"""

@tf.function
def dense_layer(x, w, b):
    return add(tf.matmul(x, w), b)

dense_layer(tf.ones([3, 2]), tf.ones([2, 2]), tf.ones([2]))


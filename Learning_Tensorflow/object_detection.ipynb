{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "object_detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0oMMOrOeD8v",
        "colab_type": "text"
      },
      "source": [
        "# Object Detection "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9rBHXNIeGQQ",
        "colab_type": "text"
      },
      "source": [
        "- Object detection is the process of localizing an object into an image by predicting the coordinates of a bounding box that contains it, while at the same time correctly classifying it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pomuKGZ4eK0w",
        "colab_type": "text"
      },
      "source": [
        " - The tasks of regressing the bounding box coordinates of a single object and classifying the content are called localization and classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgzsVPVU3toi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F9YRaQUdhsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsY0TOTj4-fX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhnPZG5G3_RJ",
        "colab_type": "code",
        "outputId": "59271f2d-4fdb-46f5-a9df-3023efdb180a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXeqX2BK3nRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train, test, validation), info = tfds.load( \"voc\", split=[\"train\", \"test\", \"validation\"], with_info=True )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Xsk4Nq34Gh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(info)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkdmPDao476r",
        "colab_type": "text"
      },
      "source": [
        "For every image, there is a SequenceDict object that contains the information of every labeled object present. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0a7ePMKj4v--",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.device(\"/GPU:0\"): \n",
        "    for row in train.take(5): \n",
        "        obj = row[\"objects\"] \n",
        "        image = tf.image.convert_image_dtype(row[\"image\"], tf.float32) \n",
        " \n",
        "        for idx in tf.range(tf.shape(obj[\"label\"])[0]): \n",
        "            image = tf.squeeze( \n",
        "                tf.image.draw_bounding_boxes( \n",
        "                    images=tf.expand_dims(image, axis=[0]), \n",
        "                    boxes=tf.reshape(obj[\"bbox\"][idx], (1, 1, 4)), \n",
        "                    colors=tf.reshape(tf.constant((1.0, 1.0, 0, 0)), (1, 4)), \n",
        "                ), \n",
        "                axis=[0], \n",
        "            ) \n",
        "\n",
        "            print( \n",
        "                \"label: \", info.features[\"objects\"][\"label\"].int2str(obj[\"label\"][idx]) \n",
        "            ) \n",
        "        plt.imshow(image)\n",
        "        plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyczwGj_J5rL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter(dataset):\n",
        "    return dataset.filter(lambda row: tf.equal(tf.shape(row[\"objects\"][\"label\"])[0], 1))\n",
        "\n",
        "train, test, validation = filter(train), filter(test), filter(validation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqZQf_XMD9vp",
        "colab_type": "text"
      },
      "source": [
        "# Object Localization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lQqwB-uD4iE",
        "colab_type": "text"
      },
      "source": [
        "- Object Localization is just a regression problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j01V5cJw6CCO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.layers.Input(shape=(299,299,3))\n",
        "net = hub.KerasLayer(\n",
        "    \"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2\",\n",
        "    output_shape = [2048],\n",
        "    trainable = False,\n",
        ") (inputs)\n",
        "\n",
        "net = tf.keras.layers.Dense(512) (net)\n",
        "net = tf.keras.layers.ReLU() (net)\n",
        "cordinates = tf.keras.layers.Dense(4, use_bias=False) (net)\n",
        "\n",
        "regressor = tf.keras.Model(inputs = inputs, outputs=cordinates)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyMeCSa6CZtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare(dataset):\n",
        "    def _fn(row):\n",
        "        row[\"image\"] = tf.image.convert_image_dtype(row[\"image\"], tf.float32)\n",
        "        row[\"image\"] = tf.image.resize(row[\"image\"], (299, 299))\n",
        "        return row\n",
        "\n",
        "    return dataset.map(_fn)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHNV0m33DB5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train, test, validation = prepare(train), prepare(test), prepare(validation)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8WJWpd1Dz0j",
        "colab_type": "text"
      },
      "source": [
        "Using the mean_squared error loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hjDB-voADaTq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def l2(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.square(y_pred - tf.squeeze(y_true, axis=[1])))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YazJ4hHvEzdm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def draw(dataset, regressor, step):\n",
        "    with tf.device(\"/CPU:0\"):\n",
        "        row = next(iter(dataset.take(3).batch(3)))\n",
        "        images = row[\"image\"]\n",
        "        obj = row[\"objects\"]\n",
        "        boxes = regressor(images)\n",
        "        tf.print(boxes)\n",
        "\n",
        "        images = tf.image.draw_bounding_boxes(\n",
        "            images=images, boxes=tf.reshape(boxes, (-1, 1, 4)), colors=[[0,0,255]]\n",
        "        )\n",
        "        images = tf.image.draw_bounding_boxes(\n",
        "            images=images, boxes=tf.reshape(obj[\"bbox\"], (-1, 1, 4)), colors=[[0,0,255]]\n",
        "        )\n",
        "        tf.summary.image(\"images\", images, step=step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlHFRUk5H_0i",
        "colab_type": "code",
        "outputId": "1de05a0a-4807-41ae-97a8-d2acf1016ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "optimizer = tf.optimizers.Adam()\n",
        "epochs = 10\n",
        "batch_size = 3\n",
        "\n",
        "global_step = tf.Variable(0, name=\"global_step\", trainable=False, dtype=tf.int64)\n",
        "\n",
        "train_writer = tf.summary.create_file_writer(\"log/train\")\n",
        "validation_writer = tf.summary.create_file_writer(\"log/test\")\n",
        "\n",
        "with validation_writer.as_default():\n",
        "    draw(validation, regressor, global_step)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.996103168 -0.690272868 0.609899282 -0.544541836]\n",
            " [-1.04774523 -1.03945589 0.701453269 -0.593110919]\n",
            " [-0.624802232 -0.333296239 0.730165124 -0.145599976]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GLlFlZY8IrLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(image, coordinates):\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss = l2(coordinates, regressor(image))\n",
        "    gradients = tape.gradient(loss, regressor.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, regressor.trainable_variables))\n",
        "    return loss\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXkozVrWL7xU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_batches = train.cache().batch(batch_size).prefetch(1)\n",
        "with train_writer.as_default():\n",
        "    for _ in tf.range(epochs):\n",
        "        for batch in train_batches:\n",
        "            obj = batch[\"objects\"]\n",
        "            coordinates = obj[\"bbox\"]\n",
        "            loss = train_step(batch[\"image\"], coordinates)\n",
        "            tf.summary.scalar(\"loss\", loss, step=global_step)\n",
        "            global_step.assign_add(1)\n",
        "            if (global_step % 10 == 0):\n",
        "                tf.print(\"step \", global_step, \" loss: \", loss)\n",
        "                with validation_writer.as_default():\n",
        "                    draw(validation, regressor, global_step)\n",
        "                with train_writer.as_default():\n",
        "                    draw(train, regressor, global_step)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90qn2f60OlRU",
        "colab_type": "text"
      },
      "source": [
        "The training loop previously defined has various problems:\n",
        "\n",
        "The only measured metric is the L2 loss\n",
        "\n",
        "The validation set is never used to measure any numerical score\n",
        "\n",
        "No check for overfitting is present\n",
        "\n",
        "There is a complete lack of a metric that measures how good the regression of the bounding box is, measured on both the training and the validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phZ7TNR1PEXs",
        "colab_type": "text"
      },
      "source": [
        "Of course, having a perfect match is not an easy task; for this reason, a function that measures how good the detected bounding box isÂ with a numerical score (with respect to the ground truth) is needed. The most widely used function to measure the goodness of localization is the Intersection over Union."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCON31efPHQH",
        "colab_type": "text"
      },
      "source": [
        "# Intersection Over Union"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ysEl5sVPQev",
        "colab_type": "text"
      },
      "source": [
        "Intersection over Union (IoU) is defined as the ratio between the area of overlap and the area of union.\n",
        "\n",
        "In practice, the IoU measures how much the predicted bounding box overlaps with the ground truth. Since IoU is a metric that uses the areas of the objects, it can be easily expressed treating the ground truth and the detected area like sets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Xv47IiTf6ev",
        "colab_type": "text"
      },
      "source": [
        "The IoU value is in the [0,1] range, where 0 is a no-match (no overlap), and 1 is the perfect match. The IoU value is used as an overlap criterion; usually, an IoU value greater than 0.5 is considered as a true positive (match), while any other value is regarded as a false positive. There are no true negatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cDb2S3jM_6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iou(pred_box, gt_box, h, w):\n",
        "    \"\"\"\n",
        "    Compute IoU between detect box and gt boxes\n",
        "    Args:\n",
        "        pred_box: shape (4,): y_min, x_min, y_max, x_max - predicted box\n",
        "        gt_boxes: shape (4,): y_min, x_min, y_max, x_max - ground truth\n",
        "        h: image height\n",
        "        w: image width\n",
        "    \"\"\"\n",
        "\n",
        "    def _swap(box):\n",
        "        return tf.stack([box[1] * w, box[0] * h, box[3] * w, box[2] * h])\n",
        "\n",
        "    pred_box = _swap(pred_box)\n",
        "    gt_box = _swap(gt_box)\n",
        "\n",
        "    box_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n",
        "    area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n",
        "\n",
        "    xx1 = tf.maximum(pred_box[0], gt_box[0])\n",
        "    yy1 = tf.maximum(pred_box[1], gt_box[1])\n",
        "    xx2 = tf.maximum(pred_box[2], gt_box[2])\n",
        "    yy2 = tf.maximum(pred_box[3], gt_box[3])\n",
        "\n",
        "    w = tf.maximum(0, xx2 - xx1)\n",
        "    h = tf.maximum(0, yy2 - yy2)\n",
        "\n",
        "    inter = w * h\n",
        "    return inter / (box_area + area - inter)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqc7NDyLm99C",
        "colab_type": "text"
      },
      "source": [
        "# Average precision\n",
        "\n",
        "A value of IoU greater than a specified threshold (usually 0.5) allows us to treat the bounding box regressed as a match.\n",
        "\n",
        "Avg_precision = TP / (TP + FP)\n",
        "\n",
        "In the object detection challenges, the Average Precision (AP) is often measured for different values of IoU. The minimum requirement is to measure the AP for an IoU value of 0.5,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XAIqHpLnhJz",
        "colab_type": "text"
      },
      "source": [
        "- Average precision and the IoU are not object-detection-specific metrics, but they can be used whenever a localization task is performed (the IoU) and the precision of the detection is measured (the mAP).\n",
        "\n",
        "- Measuring the mean average precision (over a single class) requires you to fix a threshold for the IoU measurement and to define the tf.metrics.Precision object that computes the mean average precision over the batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QjT5WGthYbk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# IoU threshold\n",
        "threshold = 0.75\n",
        "# Metric object\n",
        "precision_metric = tf.metrics.Precision()\n",
        "\n",
        "def draw(dataset, regressor, step):\n",
        "    with tf.device(\"/CPU:0\"):\n",
        "        row = next(iter(dataset.take(3).batch(3)))\n",
        "        images = row[\"image\"]\n",
        "        obj = row[\"objects\"]\n",
        "        boxes = regressor(images)\n",
        "\n",
        "        images = tf.image.draw_bounding_boxes(\n",
        "            images=images, boxes=tf.reshape(boxes, (-1, 1, 4))\n",
        "        )\n",
        "        images = tf.image.draw_bounding_boxes(\n",
        "            images=images, boxes=tf.reshape(obj[\"bbox\"], (-1, 1, 4))\n",
        "        )\n",
        "        tf.summary.image(\"images\", images, step=step)\n",
        "\n",
        "        true_labels, predicted_labels = [], []\n",
        "        for idx, predicted_box in enumerate(boxes):\n",
        "            iou_value = iou(predicted_box, tf.squeeze(obj[\"bbox\"][idx]), 299, 299)\n",
        "            true_labels.append(1)\n",
        "            predicted_labels.append(1 if iou_value >= threshold else 0)\n",
        "\n",
        "        precision_metric.update_state(true_labels, predicted_labels)\n",
        "        tf.summary.scalar(\"precision\", precision_metric.result(), step=step)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xqr69hv7ohIk",
        "colab_type": "text"
      },
      "source": [
        "# Multi-Task Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDXtTjkRo1aa",
        "colab_type": "text"
      },
      "source": [
        "In practice, multi-task learning is a machine learning subfield with the explicit goal of solving multiple different tasks, exploiting commonalities and differences across tasks. It has been empirically shown that using the same network to solve multiple tasks usually results in improved learning efficiency and prediction accuracy compared to the performance achieved by the same network trained to solve the same tasks separately.\n",
        "\n",
        "Multi-task learning also helps to fight the overfitting problem since the neural network is less likely to adapt its parameters to solve a specific task, so it has to learn how to extract meaningful features that can be useful to solve different tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k4KfsAPpY_g",
        "colab_type": "text"
      },
      "source": [
        "Using a double-headed neural network allows us to have faster inference time, since only a single forward pass of a single model is needed to achieve better performance overall."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWTN3Z4VqPAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmcT0EgNojFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = tf.keras.layers.Input(shape=(299,299,3))\n",
        "net = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2\", output_shape=[2048],\n",
        "                     trainable=False) (inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oMDqmxYpxlr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reg = tf.keras.layers.Dense(512, activation='relu') (net)\n",
        "cordinates = tf.keras.layers.Dense(4, use_bias=False) (reg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tE9fREHqBQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clasf = tf.keras.layers.Dense(512, activation='relu') (net)\n",
        "clasf = tf.keras.layers.Dense(256, activation='relu') (clasf)\n",
        "clasf = tf.keras.layers.Dense(num_classes, activation='softmax', use_bias=False) (clasf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHKiEWvHqakj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Model(inputs=inputs, outputs=[cordinates, clasf])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjD0y-qbqoA9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-n57K9orbnk",
        "colab_type": "text"
      },
      "source": [
        "Classifying images with a single object inside and regressing the coordinate of the only bounding box present can be applied only in limited real-life scenarios. More often, instead, given an input image, it is required to localize and classify multiple objects at the same time (the real object detection problem)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK3ij7hartIN",
        "colab_type": "text"
      },
      "source": [
        "# Anchor Boxes and Anchor Based Detectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz_QGhN4s94n",
        "colab_type": "text"
      },
      "source": [
        "Anchor-based detectors rely upon the concept of anchor boxes to detect objects in images in a single pass, using a single architecture.\n",
        "\n",
        "The intuitive idea of the anchor-based detectors is to split the input image into several regions of interests (the anchor boxes) and apply a localization and regression network to each of them. The idea is to make the network learn not only to regress the coordinates of a bounding box and classify its content, but also to use the same network to look at different regions of the image in a single forward pass.\n",
        "\n",
        "To train these models, it is required not only to have a dataset with the annotated ground truth boxes, but also to add to every input image a new collection of boxes that overlap (with the desired amount of IoU) the ground truth boxes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D37C7V7KtM8w",
        "colab_type": "text"
      },
      "source": [
        "Anchor-boxes are a discretization of the input image in different regions, also called anchors or bounding boxes prior. The idea behind the concept of anchor-boxes is that the input can be discretized in different regions, each of them with a different appearance. An input image could contain big and small objects, and therefore the discretization should be made at different scales in order to detect the same time objects at different resolutions.\n",
        "\n",
        "When discretizing the input in anchor boxes, the important parameters are as follows:\n",
        "\n",
        "- The grid size: How the input is evenly divided\n",
        "- The box scale levels: Given the parent box, how to resize the current box\n",
        "- The aspect ratio levels: For every box, the ratio between width and height"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJFwucboqpWr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
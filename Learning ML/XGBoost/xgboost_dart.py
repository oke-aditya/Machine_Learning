# -*- coding: utf-8 -*-
"""XGBoost_DART.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I71brSeD4M6WxuPkVoNRBEaIvDqhSP5L

# DART: Dropouts meet Multiple Additive Regression Trees.

- http://proceedings.mlr.press/v38/korlakaivinayak15.pdf

## Features

- Drop trees in order to solve the over-fitting.

- Trivial trees (to correct trivial errors) may be prevented.

Because of the randomness introduced in the training, expect the following few differences:

- Training can be slower than gbtree because the random dropout prevents usage of the prediction buffer.

- The early stop might not be stable, due to the randomness.

## Parameters

- The booster DART inherits gbtree booster, so it supports all parameters that gbtree does, such as eta, gamma, max_depth etc.

Additional parameters are noted below:

sample_type: type of sampling algorithm.

- uniform: (default) dropped trees are selected uniformly.

- weighted: dropped trees are selected in proportion to weight.

normalize_type: type of normalization algorithm.

- tree: (default) New trees have the same weight of each of dropped trees.

- forest: New trees have the same weight of sum of dropped trees (forest).

rate_drop: dropout rate.

- range: [0.0, 1.0]

skip_drop: probability of skipping dropout.

- If a dropout is skipped, new trees are added in the same manner as gbtree.

- range: [0.0, 1.0]

## Example
"""

import xgboost as xgb

dtrain = xgb.DMatrix('demo/data/agaricus.txt.train')
dtest = xgb.DMatrix('demo/data/agaricus.txt.test')

# Specify paramters via a dict (map)
param = {'booster': 'dart',
         'max_depth' : 5,
         'learning_rate' : 0.1,
         'objective' : 'binary:logistic',
         'sample_type' : 'uniform',
         'normalize_type' : 'tree',
         'rate_drop' : 0.2,
         'skip_drop' : 0.4
        }

num_rounds = 50

bst = xgb.train(params = param, dtrain = dtrain, num_boost_round = num_rounds)

# make prediction
# ntree_limit must not be 0
preds = bst.predict(dtest, ntree_limit = num_rounds)

"""## Note

- Specify ntree_limit when predicting with test sets

- By default, bst.predict() will perform dropouts on trees. To obtain correct results on test sets, disable dropouts by specifying a nonzero value for ntree_limit.
"""
# -*- coding: utf-8 -*-
"""XGBoost1.0_CuDF.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QYfjtXerNDgGB2m-AzrF15ifVtGyacRu

# Install Rapids First

- Note use only T4 or P100 Or P4 GPU which is compatible for RAPIDS.
"""

!nvidia-smi

! cat /proc/cpuinfo

# Install RAPIDS
!git clone https://github.com/rapidsai/rapidsai-csp-utils.git
!bash rapidsai-csp-utils/colab/rapids-colab.sh

import sys, os

dist_package_index = sys.path.index('/usr/local/lib/python3.6/dist-packages')
sys.path = sys.path[:dist_package_index] + ['/usr/local/lib/python3.6/site-packages'] + sys.path[dist_package_index:]
sys.path
exec(open('rapidsai-csp-utils/colab/update_modules.py').read(), globals())

"""# XGBoost 1.0 Features and Intro

New features in XGBoost 1.0

- XGBoost supports distributed Computing on Kubernetes
- We can use Kubernetes now.
- All cloud such as AWS, can use xGBoost
- Native Dask interface on XGBoost.
- Supports Cudf on RAPIDS.
- Already it supports numpy, pandas, libsvm format.
- Now it supports CUDA Dataframe.
- XGBoost works on GPU already. We can use Cuda Dataframe with end to end on GPU.
- Lot of GPU additions.
- Learning to rank is GPU accelerated.

Internally uses lambda mart ranking algorithm. Uses pairwise ranking approach by sampling many pairs.

- Scaling for multi-core CPUs is enhanced.

## Loading Data
"""

import cudf

! pip install xgboost

import xgboost as xgb
print(xgb.__version__)

import sys,tempfile, urllib, os
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np

from sklearn.datasets import fetch_openml
covtyp = fetch_openml(name='covertype', version=4)
# Predicting the forest cover type using 53 variables.
# Predict one categorical class.

covtyp.data.shape

np.unique(covtyp.target)

cov_df = pd.DataFrame(data=np.c_[covtyp['data'], covtyp['target']], columns=covtyp['feature_names'] + ['target'])

cov_df.memory_usage().sum()

cov_df.head()

cov_df.target.value_counts()

cov_df.dtypes

"""- Convert all objects into float32 format.
- Keep the categorical target as int32
"""

for cols in cov_df.columns:
    cov_df[cols] = cov_df[cols].astype(np.float32)

cov_df.dtypes

cov_df['target'] = cov_df['target'].astype(np.int32)

cov_df.dtypes

"""- Keep target variable from 0 - 7 instead of 1 - 8"""

cov_df['target'] = cov_df['target'] - 1

cov_df_x = cov_df.drop(['target'], axis=1)

cov_df_y = pd.DataFrame(cov_df['target'])

X_train, X_test, y_train, y_test = train_test_split(cov_df_x, cov_df_y, train_size=0.75, stratify=cov_df_y, random_state=31)

"""# Data GPU + XGBoost GPU"""

X_train_gdf = cudf.DataFrame.from_pandas(X_train)
X_test_gdf = cudf.DataFrame.from_pandas(X_test)
y_train_gdf = cudf.DataFrame.from_pandas(y_train)
y_test_gdf = cudf.DataFrame.from_pandas(y_test)

!nvidia-smi

# New in 1.0 we can directly use the cuda dataframe
dtrain = xgb.DMatrix(X_train_gdf, label=y_train_gdf)
dtest = xgb.DMatrix(X_test_gdf, label=y_test_gdf)

!nvidia-smi

import time

num_rounds = 200
max_depth = 10
params = {
    'colsample_bylevel' : 1,
    'colsample_bytree' : 1,
    'learning_rate' : 0.1,
    'random_state': 31,
    'objective': 'multi:softmax',
    'num_class' : 7
}

params['tree_method'] = 'gpu_hist'
params['grow_policy'] = 'depthwise'
params['max_depth'] = max_depth
params['max_leaves'] = 0
params['verbosity'] = 0
params['gpu_id'] = 0
params['updater'] = 'grow_gpu_hist'
params['predictor'] = 'gpu_predictor'

gpu_result = {}
start_time = time.time()
# Training it
xgb.train(params, dtrain, num_rounds, evals = {(dtest, 'test')}, evals_result=gpu_result, verbose_eval=20)
end_time = time.time()  
print("GPU Time taken to train = %s"%(end_time - start_time))

"""# Data RAM + XGBoost GPU

- NOTE: Not much time taken to move data from RAM to GPU
"""

# Training on GPU but with data on RAM

dtrain_cpu = xgb.DMatrix(X_train, label=y_train)
dtest_cpu = xgb.DMatrix(X_test, label=y_test)

num_rounds = 200
max_depth = 10
params = {
    'colsample_bylevel' : 1,
    'colsample_bytree' : 1,
    'learning_rate' : 0.1,
    'random_state': 31,
    'objective': 'multi:softmax',
    'num_class' : 7
}

params['tree_method'] = 'gpu_hist'
params['grow_policy'] = 'depthwise'
params['max_depth'] = max_depth
params['max_leaves'] = 0
params['verbosity'] = 0
params['gpu_id'] = 0
params['updater'] = 'grow_gpu_hist'
params['predictor'] = 'gpu_predictor'

gpu_result = {}
start_time = time.time()
# Training it
xgb.train(params, dtrain, num_rounds, evals = {(dtest, 'test')}, evals_result=gpu_result, verbose_eval=20)
end_time = time.time()  
print("RAM Time taken to train = %s" %(end_time - start_time))

! nvidia-smi

"""# Data RAM + XGBoost CPU"""

dtrain_cpu = xgb.DMatrix(X_train, label=y_train)
dtest_cpu = xgb.DMatrix(X_test, label=y_test)

params = {}

num_rounds = 200
max_depth = 10

params = {
    'colsample_bylevel' : 1,
    'colsample_bytree' : 1,
    'learning_rate' : 0.1,
    'random_state': 31,
    'objective': 'multi:softmax',
    'num_class' : 7
}

params['tree_method'] = 'hist'
params['grow_policy'] = 'depthwise'
params['max_depth'] = max_depth
params['max_leaves'] = 0
params['verbosity'] = 0
# params['gpu_id'] = 0
# params['updater'] = 'hist'
params['predictor'] = 'cpu_predictor'

cpu_result = {}
start_time = time.time()
# Training it
xgb.train(params, dtrain, num_rounds, evals = {(dtest, 'test')}, evals_result=cpu_result, verbose_eval=20)
end_time = time.time()  
print("RAM Time taken to train = %s" %(end_time - start_time))

"""# Which to use and When

- If data is not big use pandas directly
- Pandas works on single node so has limitation with bigger datasets.
- For bigger datasets like 3GB if we have GPU available use RAPIDS and directly make a CuDf it will be faster and efficient.
- Pandas will give performance issues with huge data.
- If data is not fitting into single GPU then use distributed GPU's or use distributed frameworks like Dask.
- For CPU only training one can use Intel's DAAL framework.
"""
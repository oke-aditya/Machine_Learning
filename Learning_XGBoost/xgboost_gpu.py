# -*- coding: utf-8 -*-
"""XGBoost_GPU.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a4jIysXnpNuBueByarOzX6CKtJwhx8A9

# XGBoost on GPU
---

- This is tested on XGboost 0.90 version
"""

!nvidia-smi

import xgboost as xgb

print(xgb.__version__)

import sys,tempfile, urllib, os
import pandas as pd
from sklearn.model_selection import train_test_split
import numpy as np

from sklearn.datasets import fetch_openml
covtyp = fetch_openml(name='covertype', version=4)

covtyp.data.shape

np.unique(covtyp.target)

cov_df = pd.DataFrame(data= np.c_[covtyp['data'], covtyp['target']],
                     columns= covtyp['feature_names'] + ['target'])

cov_df.memory_usage(index=True).sum()

"""# Loading Data"""

cov_df.head()

print ("Rows     : " ,cov_df.shape[0])
print ("Columns  : " ,cov_df.shape[1])

cov_df.target.value_counts()

# To use in XGboost we need the data type to be float32
# We have it as objects currently

cov_df.dtypes

for cols in cov_df.columns:
    cov_df[cols] = cov_df[cols].astype(np.float32)

cov_df.dtypes

cov_df['target'] = cov_df['target'].astype(np.int32)

# We need the values to start from 0 not from 1 for categorical variable
cov_df['target'] = cov_df['target'] - 1

cov_df_x = cov_df.drop(['target'], axis=1, inplace=False)

cov_df_y = pd.DataFrame(cov_df['target'])

X_train, X_test, y_train, y_test = train_test_split(cov_df_x, cov_df_y, test_size=0.2, random_state=31, stratify=cov_df_y)

"""# XGBoost Benchmarking"""

# For using XGBoost we need to use the DMatrix data type

dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Still we have not loaded the data into GPU
!nvidia-smi

import time

# We are using the XGboost API not the sklearn API
num_rounds = 50
max_depth = 8

params = {
  'colsample_bylevel': 1,
  'colsample_bytree': 1,
  'gamma': 0,
  'learning_rate': 0.1, 
  'random_state': 1010,
  'objective': 'multi:softmax', 
  'num_class': 7,
}

params['tree_method'] = 'hist'       
params['grow_policy'] = 'depthwise'
params['max_depth'] = max_depth
params['max_leaves'] = 0
params['verbosity'] = 0

cpu_result = {}
start_time = time.time()
xgb.train(params, dtrain, evals={(dtest, 'test')}, evals_result=cpu_result, verbose_eval=20)
end_time = time.time()
print("Training Time on CPU = %s" %(end_time - start_time))

# Doing the same on GPU
# We need to change the parameters to GPU Hist methods
params = {}
max_depth = 8

params['tree_method'] = 'gpu_hist'
params['grow_policy'] = 'depthwise'
params['max_depth'] = max_depth
params['max_leaves'] = 0
params['verbosity'] = 0
params['gpu_id'] = 0
params['updater'] = 'grow_gpu_hist'
params['predictor'] = 'gpu_predictor'

gpu_result = {}
params['updater'] = 'grow_gpu_hist'
params['predictor'] = 'gpu_predictor'

start_time = time.time()
xgb.train(params, dtrain, num_boost_round=50, evals={(dtest, 'test')}, evals_result=gpu_result, verbose_eval=20)
end_time = time.time()
print("Training Time on GPU : %s" %(end_time - start_time))

!nvidia-smi
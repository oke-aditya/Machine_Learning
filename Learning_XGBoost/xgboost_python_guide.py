# -*- coding: utf-8 -*-
"""XGBoost_Python_Guide.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AaR5_y2JARABIWha0e2ucmdbNufMrskf

# Python Guide

## Loading Data

The XGBoost python module is able to load data from:

- LibSVM text format file

- Comma-separated values (CSV) file

- NumPy 2D array

- SciPy 2D sparse array

- cuDF DataFrame

- Pandas data frame, and

- XGBoost binary buffer file.

### Loading LibSVM text file
"""

dtrain = xgb.DMatrix('train.svm.txt')
dtest = xgb.DMatrix('test.svm.buffer')

"""### Loading a CSV File

Categorical features not supported

Note that XGBoost does not provide specialization for categorical features; if your data contains categorical features, load it as a NumPy array first and then perform corresponding preprocessing steps like one-hot encoding.
"""

import xgboost as xgb

dtrain = xgb.DMatrix('train.csv?format=csv&label_column=0')
dtest = xgb.DMatrix('test.csv?format=csv&label_column=0')

"""Use Pandas to load CSV files with headers

Currently, the DMLC data parser cannot parse CSV files with headers. Use Pandas (see below) to read CSV files with headers.

### Loading Numpy Array
"""

import numpy as np

data = np.random.rand(5,10)
print(data.shape)                          # 5 rows and 10 columns
print(data)

label = np.random.randint(2, size=5)  # Binary target

dtrain = xgb.DMatrix(data, label = label)

print(dtrain)

"""### Loading Pandas DataFrame"""

import pandas as pd

df = pd.DataFrame(np.arange(12).reshape((4,3)), columns = ['a','b','c'])

df.head()

label = pd.DataFrame(np.random.randint(2, size=4))

label.head()

dtrain = xgb.DMatrix(data, label = label)

"""### Saving into XGBoost Buffer file"""

dtrain.save_binary('train.buffer')

dtrain2 = xgb.DMatrix('train.buffer')

"""## Other Stuff"""

# Missing values can be replaced by a default value in the DMatrix constructor:
dtrain = xgb.DMatrix(data, label=label, missing=-999.0)

# Weights can be set when needed:
w = np.random.rand(5, 1)
dtrain = xgb.DMatrix(data, label=label, missing=-999.0, weight=w)

"""## Setting Parameters, Training, Saving, Re-Loading, Visualization

### Parameters Setting

- XGBoost can use either a list of pairs or a dictionary to set parameters. 

For instance:

- Booster parameters
"""

param = {'max_depth': 2, 'eta': 1, 'objective' : 'binary:logistic'}
param['nthread'] = 4
param['eval_metric']  = 'auc'

# Can set multiple metrics as well
param['eval_metric'] = ['auc','rmse']

# Specify validation to watch performance
evallist = [(dtest, 'eval'), (dtrain, 'train')]

"""### Training example

- Training a model requires a parameter list and data set.
"""

df = pd.DataFrame(np.arange(12).reshape((4,3)), columns = ['a','b','c'])
label = pd.DataFrame(np.random.randint(2, size=4))

df.head()

label.head()

dtrain = xgb.DMatrix(df, label = label)

param = {'max_depth' : 2, 'eta' : 0.2, 'objective' : 'binary:logistic', 'eval_metric' : 'error'}

num_round = 10

bst = xgb.train(params = param, dtrain = dtrain, num_boost_round=num_round)

# After training, the model can be saved.
bst.save_model('save_model.model')

# dumping model as text file
bst.dump_model('dump.raw.txt')

# dumping model with feature map
bst.dump_model('dump.raw.txt', 'featmap.txt')

bst = xgb.Booster()
bst.load_model('/content/save_model.model')
print(bst)

"""### Early Stopping

If you have a validation set, you can use early stopping to find the optimal number of boosting rounds. Early stopping requires at least one set in evals. If thereâ€™s more than one, it will use the last.
"""

bst = xgb.train(params = param, dtrain = dtrain, num_boost_round=num_round, early_stopping_rounds=2)

"""The model will train until the validation score stops improving. Validation error needs to decrease at least every early_stopping_rounds to continue training.

- If early stopping occurs, the model will have three additional fields: bst.best_score, bst.best_iteration and bst.best_ntree_limit. Note that xgboost.train() will return a model from the last iteration, not the best one.

### Predictions from Model

- Trained Model can be used to make predictions on dataset
"""

ypred = bst.predict(dtest)

"""- If early stopping is enabled you can get predictions from the best iteration with bst.best_ntree_limit"""

ypred = bst.predict(dtest, ntree_limit=bst.best_ntree_limit)

"""### Plotting

You can use plotting module to plot importance and output tree.

To plot importance, use xgboost.plot_importance(). This function requires matplotlib to be installed.
"""

xgb.plot_importance(bst)

"""To plot the output tree via matplotlib, use xgboost.plot_tree(), specifying the ordinal number of the target tree. This function requires graphviz and matplotlib."""

xgb.plot_tree(bst, num_trees=2)

"""When you use IPython, you can use the xgboost.to_graphviz() function, which converts the target tree to a graphviz instance. The graphviz instance is automatically rendered in IPython."""

xgb.to_graphviz(bst, num_trees=2)

"""## Parameter Tuning

- Use the concept of Bias-Variance TradeOff

### Control Overfitting

When you observe high training accuracy, but low test accuracy,it is likely that you encountered overfitting problem.

There are in general two ways that you can control overfitting in XGBoost:

- The first way is to directly control model complexity.

- This includes max_depth, min_child_weight and gamma.

- The second way is to add randomness to make training robust to noise.

- This includes subsample and colsample_bytree.

- You can also reduce stepsize eta. Remember to increase num_round when you do so.

### Handle Imbalanced Dataset

For common cases such as ads clickthrough log, the dataset is extremely imbalanced. This can affect the training of XGBoost model, and there are two ways to improve it.

If you care only about the overall performance metric (AUC) of your prediction

- Balance the positive and negative weights via scale_pos_weight

- Use AUC for evaluation

If you care about predicting the right probability

- In such a case, you cannot re-balance the dataset

- Set parameter max_delta_step to a finite number (say 1) to help convergence

Parameter tuning is art use the following webpage and master

https://xgboost.readthedocs.io/en/latest/parameter.html

### GPUs for XGBoost

- Can be used.
- Specify the tree_method parameter as 'gpu_hist'

Equivalent to the XGBoost fast histogram algorithm. Much faster and uses considerably less memory. NOTE: Will run very slowly on GPUs older than Pascal architecture.

- Faster performance.
"""
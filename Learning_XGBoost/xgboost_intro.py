# -*- coding: utf-8 -*-
"""XGBoost_Intro.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BxEyxXKKJqIbtldyGkM1V5jAPzXJQdZK

# Quick Start with XGBoost

https://xgboost.readthedocs.io/en/latest/get_started.html

- Advanced Library for Boosted Trees.
- GPU Support.
- Can help you win a Kaggle Competition.
- Can deploy ML products.
"""

!pip install xgboost

"""## Quick Tutorial"""

import xgboost as xgb

"""- XGBoost uses Data Matrix as an internal data structure for fast computing.
- It can be constructed using numpy arrays as well

- For a Binary Classification task we can quickly use XGB as follows
"""

dtrain = xgb.DMatrix('demo/data/agaricus.txt.train')
dtest = xgb.DMatrix('demo/data/agaricus.txt.test')

# Specify the parameters via map 
param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}
num_rounds = 2

bst = xgb.train(params=param, dtrain = dtrain, num_boost_round=num_rounds)

preds = bst.predidct(dtest)

"""# Quick Guide

- XGBoost stands for “Extreme Gradient Boosting”, where the term “Gradient Boosting” originates from the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. 

- XGBoost is used for supervised learning problems, where we use the training $x_i$ data (with multiple features) to predict a target variable $y_i$

- Theory tutorial in below link by author of XGBoost
- https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf

- XGBoost is exactly a tool motivated by the formal principle introduced in this tutorial! More importantly, it is developed with both deep consideration in terms of systems optimization and principles in machine learning. 

- The goal of this library is to push the extreme of the computation limits of machines to provide a scalable, portable and accurate library.

- For more detailed stuff https://github.com/dmlc/xgboost/tree/master/demo
"""
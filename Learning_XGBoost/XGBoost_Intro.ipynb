{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBoost_Intro.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q3KigyPPjV9",
        "colab_type": "text"
      },
      "source": [
        "# Quick Start with XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GMR0prtQP0MW",
        "colab_type": "text"
      },
      "source": [
        "https://xgboost.readthedocs.io/en/latest/get_started.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiEcmHkOPl4O",
        "colab_type": "text"
      },
      "source": [
        "- Advanced Library for Boosted Trees.\n",
        "- GPU Support.\n",
        "- Can help you win a Kaggle Competition.\n",
        "- Can deploy ML products."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGYeHNEONH36",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "f627a480-04e7-46f3-be47-9d54d1ead68c"
      },
      "source": [
        "!pip install xgboost"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.6/dist-packages (0.90)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from xgboost) (1.17.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VPmh21XP19r",
        "colab_type": "text"
      },
      "source": [
        "## Quick Tutorial"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UReUdruGNv19",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import xgboost as xgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32L2eWWLQoIH",
        "colab_type": "text"
      },
      "source": [
        "- XGBoost uses Data Matrix as an internal data structure for fast computing.\n",
        "- It can be constructed using numpy arrays as well"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5zaxxC2RfqF",
        "colab_type": "text"
      },
      "source": [
        "- For a Binary Classification task we can quickly use XGB as follows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gostmLnSQbt8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dtrain = xgb.DMatrix('demo/data/agaricus.txt.train')\n",
        "dtest = xgb.DMatrix('demo/data/agaricus.txt.test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSvLBLaFRM1P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Specify the parameters via map \n",
        "param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}\n",
        "num_rounds = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8qo106iRmnk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bst = xgb.train(params=param, dtrain = dtrain, num_boost_round=num_rounds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YADSsXP8R9Rr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preds = bst.predidct(dtest)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8h2dOG_SPCl",
        "colab_type": "text"
      },
      "source": [
        "# Quick Guide"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpIwbHrtScar",
        "colab_type": "text"
      },
      "source": [
        "- XGBoost stands for “Extreme Gradient Boosting”, where the term “Gradient Boosting” originates from the paper Greedy Function Approximation: A Gradient Boosting Machine, by Friedman. \n",
        "\n",
        "- XGBoost is used for supervised learning problems, where we use the training $x_i$ data (with multiple features) to predict a target variable $y_i$\n",
        "\n",
        "- Theory tutorial in below link by author of XGBoost\n",
        "- https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf\n",
        "\n",
        "- XGBoost is exactly a tool motivated by the formal principle introduced in this tutorial! More importantly, it is developed with both deep consideration in terms of systems optimization and principles in machine learning. \n",
        "\n",
        "- The goal of this library is to push the extreme of the computation limits of machines to provide a scalable, portable and accurate library. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry5ZKom9SWNY",
        "colab_type": "text"
      },
      "source": [
        "- For more detailed stuff https://github.com/dmlc/xgboost/tree/master/demo"
      ]
    }
  ]
}
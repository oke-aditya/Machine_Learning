# -*- coding: utf-8 -*-
"""Spark_on_Colab.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Dt_-WO2rtwI4oX0s1rESyIM3P-l4vyyD

# Running Apache Spark on Google Colab

## Installing
"""

# Spark is written in scala. We need java and jvm for that.
# Install jvm using java 8
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
# Downloading apache spark 2.4.5
# Choose the package from downloads.apahce.org/spark
!wget -q https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz
# Extract the tar file
!tar xf spark-2.4.5-bin-hadoop2.7.tgz
# Install findspark package
# Allows to findspark and set the system variable
# Pyspark isn't on sys path by default to add that and use as a normal libraray we install findspark.
!pip install -q findspark

! ls /usr/lib/jvm

# This is optional.
# To bring data from spark df to pandas df.
# Pyspark uses py4j binding. it sends data through pickle to driver and unpickles it.
# Serialization is expensive. We can use pyarrow to transfer the objects. Pyarrow is compatible with both sparkdf and pandas df
! pip install pyarrow

"""## Set the paths"""

# Set the java home and the spark home
import os
os.environ['JAVA_HOME'] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ['SPARK_HOME'] = "/content/spark-2.4.5-bin-hadoop2.7"

import findspark
# findspark will set it in system path.
findspark.init()

# We need spark session and spark context to run
from pyspark.sql import SparkSession
# We do not have distributed environ, driver and executor node are same.
spark = SparkSession.builder.master("local[*]").getOrCreate()

# set the config for spark. # We can set the memory for both driver and executor.
spark.conf.set("spark.executor.memory", "4g")
spark.conf.set("spark.driver.memory", "4g")
spark.conf.set("spark.memory.fraction", "0.9")

"""# Submitting jdk or py files

- This is for submitting spark files in pyspark

os.environ['PYSPARK_SUBMIT_ARGS'] = 

spark.spark.Context.addPyFile('../')

# Testing
"""

import sys, tempfile, urllib

import ssl
# Work around for the ssl error.
# Ignore if it works properly
ssl._create_default_https_context = ssl._create_unverified_context

BASE_DIR = "/tmp"
OUTPUT_FILE = os.path.join(BASE_DIR, 'credit_data.csv')

credit_data = urllib.request.urlretrieve("https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data", OUTPUT_FILE)

!ls /tmp

credit_df = spark.read.option("inferSchema", "true").csv("/tmp/credit_data.csv", header=False)

credit_df.show()

credit_df.count()
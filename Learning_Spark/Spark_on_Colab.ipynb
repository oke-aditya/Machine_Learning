{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_on_Colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw2dnMNuwKjp",
        "colab_type": "text"
      },
      "source": [
        "# Running Apache Spark on Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kazftSK0H57",
        "colab_type": "text"
      },
      "source": [
        "## Installing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GbKhZdfw2it",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spark is written in scala. We need java and jvm for that.\n",
        "# Install jvm using java 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# Downloading apache spark 2.4.5\n",
        "# Choose the package from downloads.apahce.org/spark\n",
        "!wget -q https://downloads.apache.org/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "# Extract the tar file\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "# Install findspark package\n",
        "# Allows to findspark and set the system variable\n",
        "# Pyspark isn't on sys path by default to add that and use as a normal libraray we install findspark.\n",
        "!pip install -q findspark"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb2DIFg7xoJa",
        "colab_type": "code",
        "outputId": "22034ceb-ab7b-42a9-c233-0d19766e3167",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "! ls /usr/lib/jvm"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "default-java\t\t   java-11-openjdk-amd64     java-8-openjdk-amd64\n",
            "java-1.11.0-openjdk-amd64  java-1.8.0-openjdk-amd64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dghwxPojyklt",
        "colab_type": "code",
        "outputId": "5675d6ff-f0e0-4837-f31f-0c7597517fdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# This is optional.\n",
        "# To bring data from spark df to pandas df.\n",
        "# Pyspark uses py4j binding. it sends data through pickle to driver and unpickles it.\n",
        "# Serialization is expensive. We can use pyarrow to transfer the objects. Pyarrow is compatible with both sparkdf and pandas df\n",
        "! pip install pyarrow"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.6/dist-packages (0.14.1)\n",
            "Requirement already satisfied: six>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqxp1bwm0KOS",
        "colab_type": "text"
      },
      "source": [
        "## Set the paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfXKXom1zF1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the java home and the spark home\n",
        "import os\n",
        "os.environ['JAVA_HOME'] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ['SPARK_HOME'] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzHCbw2TxXU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "# findspark will set it in system path.\n",
        "findspark.init()\n",
        "\n",
        "# We need spark session and spark context to run\n",
        "from pyspark.sql import SparkSession\n",
        "# We do not have distributed environ, driver and executor node are same.\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "\n",
        "# set the config for spark. # We can set the memory for both driver and executor.\n",
        "spark.conf.set(\"spark.executor.memory\", \"4g\")\n",
        "spark.conf.set(\"spark.driver.memory\", \"4g\")\n",
        "spark.conf.set(\"spark.memory.fraction\", \"0.9\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SghDYivs0QAy",
        "colab_type": "text"
      },
      "source": [
        "# Submitting jdk or py files\n",
        "\n",
        "- This is for submitting spark files in pyspark\n",
        "\n",
        "os.environ['PYSPARK_SUBMIT_ARGS'] = \n",
        "\n",
        "spark.spark.Context.addPyFile('../')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXDHoavWK_Cs",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzYrid_67qW2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys, tempfile, urllib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7cBkQ7SLwct",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import ssl\n",
        "# Work around for the ssl error.\n",
        "# Ignore if it works properly\n",
        "ssl._create_default_https_context = ssl._create_unverified_context"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEzM50RaLDvC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BASE_DIR = \"/tmp\"\n",
        "OUTPUT_FILE = os.path.join(BASE_DIR, 'credit_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBwg9uaULJnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "credit_data = urllib.request.urlretrieve(\"https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\", OUTPUT_FILE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6Ulb54VLW5s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "9b70cec8-d593-4a28-e9bf-bb080b5132ff"
      },
      "source": [
        "!ls /tmp"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "blockmgr-3aad7eb6-02a8-431b-a298-f3cf76b60a3e\n",
            "credit_data.csv\n",
            "hsperfdata_root\n",
            "spark-2b5d23b9-b4bf-4b8b-ab3b-00249d299452\n",
            "spark-9366ec80-6c82-47c0-b884-c2f881042385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mriw6OiBNPwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "credit_df = spark.read.option(\"inferSchema\", \"true\").csv(\"/tmp/credit_data.csv\", header=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCs3lOG-Nqby",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "outputId": "77c01f0f-7651-4c6a-a1a0-f47359f637d1"
      },
      "source": [
        "credit_df.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----+------+---+---+---+---+-----+---+---+----+----+----+-----+-----+----+\n",
            "|_c0|  _c1|   _c2|_c3|_c4|_c5|_c6|  _c7|_c8|_c9|_c10|_c11|_c12| _c13| _c14|_c15|\n",
            "+---+-----+------+---+---+---+---+-----+---+---+----+----+----+-----+-----+----+\n",
            "|  b|30.83|   0.0|  u|  g|  w|  v| 1.25|  t|  t|   1|   f|   g|00202|    0|   +|\n",
            "|  a|58.67|  4.46|  u|  g|  q|  h| 3.04|  t|  t|   6|   f|   g|00043|  560|   +|\n",
            "|  a|24.50|   0.5|  u|  g|  q|  h|  1.5|  t|  f|   0|   f|   g|00280|  824|   +|\n",
            "|  b|27.83|  1.54|  u|  g|  w|  v| 3.75|  t|  t|   5|   t|   g|00100|    3|   +|\n",
            "|  b|20.17| 5.625|  u|  g|  w|  v| 1.71|  t|  f|   0|   f|   s|00120|    0|   +|\n",
            "|  b|32.08|   4.0|  u|  g|  m|  v|  2.5|  t|  f|   0|   t|   g|00360|    0|   +|\n",
            "|  b|33.17|  1.04|  u|  g|  r|  h|  6.5|  t|  f|   0|   t|   g|00164|31285|   +|\n",
            "|  a|22.92|11.585|  u|  g| cc|  v| 0.04|  t|  f|   0|   f|   g|00080| 1349|   +|\n",
            "|  b|54.42|   0.5|  y|  p|  k|  h| 3.96|  t|  f|   0|   f|   g|00180|  314|   +|\n",
            "|  b|42.50| 4.915|  y|  p|  w|  v|3.165|  t|  f|   0|   t|   g|00052| 1442|   +|\n",
            "|  b|22.08|  0.83|  u|  g|  c|  h|2.165|  f|  f|   0|   t|   g|00128|    0|   +|\n",
            "|  b|29.92| 1.835|  u|  g|  c|  h|4.335|  t|  f|   0|   f|   g|00260|  200|   +|\n",
            "|  a|38.25|   6.0|  u|  g|  k|  v|  1.0|  t|  f|   0|   t|   g|00000|    0|   +|\n",
            "|  b|48.08|  6.04|  u|  g|  k|  v| 0.04|  f|  f|   0|   f|   g|00000| 2690|   +|\n",
            "|  a|45.83|  10.5|  u|  g|  q|  v|  5.0|  t|  t|   7|   t|   g|00000|    0|   +|\n",
            "|  b|36.67| 4.415|  y|  p|  k|  v| 0.25|  t|  t|  10|   t|   g|00320|    0|   +|\n",
            "|  b|28.25| 0.875|  u|  g|  m|  v| 0.96|  t|  t|   3|   t|   g|00396|    0|   +|\n",
            "|  a|23.25| 5.875|  u|  g|  q|  v| 3.17|  t|  t|  10|   f|   g|00120|  245|   +|\n",
            "|  b|21.83|  0.25|  u|  g|  d|  h|0.665|  t|  f|   0|   t|   g|00000|    0|   +|\n",
            "|  a|19.17| 8.585|  u|  g| cc|  h| 0.75|  t|  t|   7|   f|   g|00096|    0|   +|\n",
            "+---+-----+------+---+---+---+---+-----+---+---+----+----+----+-----+-----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyghR6noNsBL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "84af6658-1377-4eb9-adf8-49eda6616bc2"
      },
      "source": [
        "credit_df.count()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "690"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    }
  ]
}
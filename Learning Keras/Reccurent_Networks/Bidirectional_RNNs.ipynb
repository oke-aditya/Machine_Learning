{"cells":[{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"# Bidirectional Recurrent Neural Networks"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## Introduction"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"- A bidirectional RNN is a common RNN variant that can offer greater performance than a regular\nRNN on certain tasks. \n- Itâ€™s frequently used in natural-language processing\n- You could call it the Swiss Army knife of deep learning for natural-language processing."},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"- RNNs are notably order dependent, or time dependent: they process the timesteps\nof their input sequences in order, and shuffling or reversing the timesteps can completely\nchange the representations the RNN extracts from the sequence. \n- This is precisely the reason they perform well on problems where order is meaningful, such as\nthe temperature-forecasting problem. \n- A bidirectional RNN exploits the order sensitivity of RNNs:\n- it consists of using two regular RNNs, such as the GRU and LSTM layers  each of which processes the input sequence in one direction (chronologically and antichronologically), and then merging their representations.\n- By processing a sequence both ways, a bidirectional RNN can catch patterns that may be overlooked by a unidirectional RNN"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## Reversed Order GRU"},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":"from keras import layers\nfrom keras.preprocessing import sequence\nfrom keras.datasets import imdb\nfrom keras.models import Sequential"},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":"max_features = 1000\nmaxlen = 50"},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":"(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)"},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":"x_padded_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_padded_test = sequence.pad_sequences(x_test, maxlen=maxlen)"},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":"x_rev_train =[x[::-1] for x in x_train]\nx_rev_test = [x[::-1] for x in x_test]"},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":"x_rev_train_padded = sequence.pad_sequences(x_rev_train, maxlen=maxlen)\nx_rev_test_padded = sequence.pad_sequences(x_rev_test, maxlen=maxlen)"},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":"model = Sequential()\nmodel.add(layers.Embedding(max_features, 128))\nmodel.add(layers.GRU(32, activation='tanh', recurrent_dropout=0.3, dropout = 0.1))\nmodel.add(layers.Dense(1, activation='sigmoid'))"},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":"model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics=['acc'])"},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Train on 25000 samples, validate on 25000 samples\nEpoch 1/2\n25000/25000 [==============================] - 26s 1ms/step - loss: 0.6189 - acc: 0.6475 - val_loss: 0.5620 - val_acc: 0.7049\nEpoch 2/2\n25000/25000 [==============================] - 26s 1ms/step - loss: 0.5379 - acc: 0.7291 - val_loss: 0.5273 - val_acc: 0.7301\n"},{"data":{"text/plain":"<keras.callbacks.History at 0x234848ef208>"},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":"model.fit(x_rev_train_padded, y_train, validation_data=(x_rev_test_padded, y_test), batch_size=64, epochs=2, verbose=1)"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"- A reverse order GRU works as good as a chrnonological order GRU.\n- We can expect this as it does not matter for a task such as sentiment classification. \n- But the features learnt by the GRU are completely different than that of a chrnonological GRU.\n- We can exploit this idea."},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"## Why bidirectional RNNs"},{"cell_type":"markdown","execution_count":null,"metadata":{},"outputs":[],"source":"- A bidirectional RNN exploits this idea to improve on the performance of chronologicalorder\nRNNs. \n- It looks at its input sequence both ways (see figure 6.25), obtaining potentially\nricher representations and capturing patterns that may have been missed by the\nchronological-order version alone."},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":"model = Sequential()\nmodel.add(layers.Embedding(max_features, 32))\nmodel.add(layers.Bidirectional(layers.GRU(32, activation='tanh', dropout=0.3, recurrent_dropout=0.2)))\nmodel.add(layers.Dense(1, activation='sigmoid'))"},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"Train on 25000 samples, validate on 25000 samples\nEpoch 1/2\n25000/25000 [==============================] - 23s 902us/step - loss: 0.5620 - acc: 0.6950 - val_loss: 0.4647 - val_acc: 0.7772\nEpoch 2/2\n25000/25000 [==============================] - 22s 862us/step - loss: 0.4551 - acc: 0.7863 - val_loss: 0.4560 - val_acc: 0.7847\n"}],"source":"model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['acc'])\nhistory = model.fit(x_padded_train, y_train, validation_data=(x_padded_test, y_test), verbose = 1, epochs=2, batch_size=64) "},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":""}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3}},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3}}